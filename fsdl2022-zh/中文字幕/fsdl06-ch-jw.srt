0
00:00:00,000 --> 00:00:02,540
大家好。欢迎回到全栈深度学习。

1
00:00:02,760 --> 00:00:08,540
这周 我们将讨论持续学习 在我看来 这是我们这门课中最令人兴奋的话题之一

2
00:00:08,800 --> 00:00:13,400
持续学习描述了在模型投入生产后对其进行迭代的过程。

3
00:00:13,560 --> 00:00:22,910
因此，使用生产数据重新训练模型有两个目的，第一，使模型适应训练模型后现实世界中发生的任何变化。

4
00:00:23,070 --> 00:00:26,710
第二 使用来自现实世界的数据来改进你的模型

5
00:00:26,870 --> 00:00:35,470
让我们深入探讨持续学习的核心理由是，与学术界不同，在现实世界中，我们从不处理静态数据分布。

6
00:00:35,630 --> 00:00:46,670
所以这意味着，如果你想使用机器学习生产，如果你想建立一个好的机器学习驱动的产品，你需要把你的目标看作是建立一个持续学习的系统，而不仅仅是建立一个静态模型。

7
00:00:46,690 --> 00:00:51,710
所以我想我们都希望这是我们之前在课堂上描述过的数据迭代。

8
00:00:51,870 --> 00:00:57,230
所以当你有更多的用户 这些用户带来更多的数据 你可以用这些数据来做更好的模型

9
00:00:57,390 --> 00:01:01,550
这种更好的模式可以帮助你吸引更多的用户 并随着时间的推移建立更好的模式

10
00:01:01,710 --> 00:01:08,830
最自动化的版本 最乐观的版本 被AndreCarpathy称为手术假期

11
00:01:08,990 --> 00:01:17,920
如果我们让我们的持续学习系统足够好 那么随着时间的推移 它会自己变得更好 而我们 作为机器学习工程师 可以去度假 当我们回来的时候 这个模型会更好

12
00:01:18,080 --> 00:01:20,240
但现实是完全不同的

13
00:01:20,340 --> 00:01:21,640
我觉得一开始还不错

14
00:01:21,800 --> 00:01:27,050
所以我们收集一些数据 我们清理并标记这些数据 我们在数据上训练一个模型

15
00:01:27,230 --> 00:01:32,250
然后我们评估这个模型 然后我们循环训练这个模型 让它基于我们所做的评估变得更好

16
00:01:32,410 --> 00:01:36,770
最后 我们完成了 我们有了一个最小可行的模型 我们准备把它投入生产

17
00:01:36,930 --> 00:01:45,530
所以我们部署了它 问题在我们部署它之后就开始了 那就是我们通常没有一个很好的方法来衡量我们的模型在生产环境中的实际表现

18
00:01:45,690 --> 00:01:53,290
所以我们经常做的就是抽查一些预测，看看它是否在做它应该做的事情，如果它看起来有效，那就太好了。

19
00:01:53,450 --> 00:01:55,550
我们可能会继续做其他项目

20
00:01:55,730 --> 00:01:57,570
也就是说 直到第一个问题出现

21
00:01:57,730 --> 00:02:02,800
现在 不幸的是 我作为一个机器学习工程师 可能不是那个发现这个问题的人

22
00:02:02,960 --> 00:02:13,510
你知道，可能是一些业务用户或项目经理意识到，嘿，我们收到了用户的投诉，或者我们有一个指标下降了，这导致了调查。

23
00:02:13,670 --> 00:02:18,170
这已经给公司造成了损失 因为产品和业务团队都必须调查这个问题

24
00:02:18,470 --> 00:02:22,990
最终，他们会把问题指向我，指向我负责的模型。

25
00:02:23,150 --> 00:02:29,770
在这一点上，我有点困在做一些临时分析，因为我不知道模型失败的原因是什么。

26
00:02:30,030 --> 00:02:39,850
也许我几个星期或几个月都没看这个模型，也许最终我能运行一堆sql查询，把一些jupyter notebook粘在一起，找出我认为的问题是什么。

27
00:02:40,000 --> 00:02:46,400
所以我会重新训练模型，重新部署它，如果幸运的话，我们可以运行一个AB测试，如果这个AB测试看起来不错，那么我们就会部署它，

28
00:02:46,400 --> 00:02:51,700
我们又回到了起点，没有得到关于模型在生产中的实际表现的持续反馈。

29
00:02:51,800 --> 00:03:02,900
所有这一切的结果是 持续学习实际上是生产机器学习生命周期中最不容易理解的部分 而且今天很少有公司在生产中做得很好

30
00:03:03,330 --> 00:03:07,130
所以这个讲师 在某些方面 会感觉和其他的讲座有些不同

31
00:03:07,290 --> 00:03:14,740
这节课的很大一部分重点是我们认为你们应该如何看待持续学习问题的结构

32
00:03:14,900 --> 00:03:23,380
这是 你知道 我们在这里所说的一些将是很好理解的行业最佳实践 其中一些将是我们对我们认为这应该是什么样子的看法

33
00:03:23,540 --> 00:03:33,990
我将向你们提供很多信息关于持续学习过程的每个不同步骤，如何考虑改进，一旦你的第一个模型投入生产，你如何做这些部分。

34
00:03:34,150 --> 00:03:39,020
和往常一样 我会就如何务实地做到这一点以及如何逐步采用它提供一些建议

35
00:03:39,180 --> 00:03:44,060
首先 我想给出一个我认为你们应该如何看待持续学习的观点

36
00:03:44,220 --> 00:03:52,600
因此 我将持续学习定义为训练一系列能够适应生产中不断出现的数据流的模型

37
00:03:52,760 --> 00:03:56,100
你可以把持续学习看作是你训练过程中的一个外部循环

38
00:03:56,440 --> 00:04:01,540
在循环的一端是您的应用程序 它由一个模型和一些其他代码组成

39
00:04:01,540 --> 00:04:09,600
用户通过提交请求 获得预测结果 然后提交关于模型在提供预测方面做得如何的反馈 与该应用程序进行交互

40
00:04:09,760 --> 00:04:13,820
持续的学习循环从记录开始 这是我们将所有数据放入循环的方式

41
00:04:14,030 --> 00:04:21,030
然后我们有数据管理触发器来进行再训练过程 数据集形成来选择数据进行再训练

42
00:04:21,190 --> 00:04:22,510
我们有自己的训练过程

43
00:04:22,670 --> 00:04:27,990
然后我们进行离线测试 这就是我们如何验证重新训练的模型是否足够好 可以投入生产

44
00:04:28,150 --> 00:04:40,370
在部署之后，我们进行在线测试，然后将模型的下一个版本带入生产，在那里我们可以重新开始循环，每个阶段都将输出传递到下一步，并且定义输出的方式是使用一组规则。

45
00:04:40,750 --> 00:04:43,990
所有这些规则合起来就是再训练策略

46
00:04:44,150 --> 00:04:48,550
接下来 我们将讨论每个阶段的再训练策略定义以及输出是什么样子的

47
00:04:48,710 --> 00:04:54,630
在记录阶段 再训练策略要回答的关键问题是 我们应该存储哪些数据

48
00:04:54,790 --> 00:05:04,120
最后 我们有一个无限的潜在标签数据流 这些数据来自生产 能够用于管理阶段的下游分析

49
00:05:04,300 --> 00:05:08,950
我们需要定义的关键规则是，从无限的数据流中，我们要优先考虑哪些数据来进行标记和潜在的再训练?

50
00:05:09,400 --> 00:05:22,660
在这个阶段的最后，将有一个有限数量的候选训练点的储存器，这些训练点有标签，并且完全准备好在再训练触发阶段反馈到训练过程中。

51
00:05:22,820 --> 00:05:25,800
要回答的关键问题是 我们应该在什么时候进行再训练

52
00:05:25,880 --> 00:05:31,180
我们知道什么时候该按下再训练按钮 而阶段的输出是启动再训练工作的信号

53
00:05:31,340 --> 00:05:42,660
在数据集形成阶段 我们需要定义的关键规则是 从整个数据库中 我们要为这个特定的训练工作训练哪些特定的数据子集

54
00:05:42,800 --> 00:05:51,700
你可以把它的输出看作是对存储库的一个视图，或者是训练数据，它指定了在离线测试阶段将用于训练工作的确切数据点，

55
00:05:51,700 --> 00:05:56,300
我们需要定义的关键规则是，对于我们所有的利益相关者来说，什么是足够好的?

56
00:05:56,530 --> 00:05:58,890
我们怎样才能确定这个模型已经准备好部署了呢

57
00:05:59,050 --> 00:06:09,650
这个阶段的输出看起来就像一个拉取请求，你的模型的成绩单，作为一个明确的签字过程，一旦你签字，新模型将投入生产。

58
00:06:09,810 --> 00:06:17,380
最后 在部署 在线测试阶段 我们需要定义的关键规则是 我们如何知道这个部署是否最成功

59
00:06:17,600 --> 00:06:24,360
这个阶段的输出将是一个信号 让你在一个理想的世界里把这个模型完全推向你的所有用户

60
00:06:24,520 --> 00:06:39,820
我认为 一旦我们部署了模型的第一个版本 我们应该把自己视为机器学习工程师的角色 而不是直接对模型进行再训练 而是坐在再训练策略之上 照看该策略 并尝试随着时间的推移改进策略本身

61
00:06:39,980 --> 00:06:50,690
因此 我们不是日复一日地训练模型 而是关注策略的运行情况 它如何解决改进模型的任务 或者对世界变化做出反应的时间

62
00:06:50,850 --> 00:06:58,330
我们提供的输入是通过调整策略 通过改变构成策略的规则 来帮助策略更好地解决任务

63
00:06:58,490 --> 00:07:02,890
这是对我们作为当今世界ml工程师角色的目标状态的描述。

64
00:07:03,050 --> 00:07:10,330
对我们大多数人来说 我们的工作在高水平上并没有这样的感觉 因为对我们大多数人来说 我们的再训练策略只是在我们想要的时候再训练模型

65
00:07:10,490 --> 00:07:12,110
这实际上并没有看起来那么糟糕

66
00:07:12,370 --> 00:07:14,130
你可以从临时的再训练中得到很好的结果。

67
00:07:14,290 --> 00:07:23,470
但是当你开始能够得到真正一致的结果时 当你重新训练模型时 并且你不再每天都在模型上工作时 那么就值得开始添加一些自动化

68
00:07:23,730 --> 00:07:32,010
或者 如果您发现自己需要每周不止一次地重新训练模型 或者甚至比这更频繁 以处理现实世界中不断变化的结果

69
00:07:32,170 --> 00:07:35,070
为了节省时间 投资自动化也是值得的

70
00:07:35,250 --> 00:07:44,610
你应该考虑的第一个基线再培训策略是定期再培训，这是你在短期内大多数情况下最终要做的。

71
00:07:44,770 --> 00:07:47,350
我们来描述一下这种周期性再训练策略

72
00:07:47,700 --> 00:07:56,800
因此，在日志记录阶段，我们将简单地记录所有内容。策展人将从我们记录的数据中随机抽样，直到我们得到我们能够处理的最大数据点数，

73
00:07:56,800 --> 00:08:01,400
我们能够标记，或者真正能够训练，然后我们将使用一些自动化工具标记它们。

74
00:08:01,600 --> 00:08:07,680
我们的再训练触发器将是周期性的 所以我们将每周训练一次 但我们将在上个月的数据上进行 例如

75
00:08:07,840 --> 00:08:16,720
然后我们将在每次训练后计算测试集的准确性，设置一个阈值，或者更可能的是，每次手动检查结果并抽检一些预测。

76
00:08:16,880 --> 00:08:24,680
然后当我们部署模型时 我们会做现场评估 在一些单独的预测上部署模型 只是为了确保事情看起来是健康的 然后继续前进

77
00:08:24,840 --> 00:08:29,260
这个基线看起来就像大多数公司在现实世界中为自动再训练所做的那样

78
00:08:29,650 --> 00:08:35,330
定期再训练是一个很好的基准 事实上 当你准备好开始进行自动再训练时 我建议你这样做

79
00:08:35,490 --> 00:08:37,500
但这并不适用于所有情况

80
00:08:37,660 --> 00:08:39,400
我们来谈谈一些失效模式

81
00:08:39,700 --> 00:08:45,310
第一类失效模式是当你拥有的数据比你能记录的多或者不能生成的多

82
00:08:45,400 --> 00:08:54,600
如果你有大量的数据，你可能需要更加小心你采样和丰富的数据，特别是如果这些数据来自长尾分布，

83
00:08:54,600 --> 00:09:04,700
你的模型需要在边缘情况下表现良好，但这些边缘情况可能无法通过标准均匀随机抽样来捕获，或者如果数据标记成本很高，

84
00:09:04,700 --> 00:09:10,500
就像在人在循环的场景中一样，您需要自定义标签规则，或者数据标注是产品的一部分。

85
00:09:10,670 --> 00:09:22,350
在长尾分布或人工循环设置这两种情况下，您可能需要更加谨慎地考虑要记录和丰富的数据子集。

86
00:09:22,510 --> 00:09:26,470
第二类可能失败的地方与管理再培训的成本有关。

87
00:09:26,630 --> 00:09:34,770
如果你的模型重新训练真的很昂贵 那么定期重新训练它可能不是最经济有效的方法 特别是如果你每次都在一个滚动的数据窗口上进行训练

88
00:09:34,930 --> 00:09:39,210
假设你每周重新训练你的模型 但你的数据实际上每天都在变化

89
00:09:39,370 --> 00:09:42,690
如果不进行更频繁的再训练 你就会失去很多表现

90
00:09:42,850 --> 00:09:48,210
你可以增加频率 每隔几个小时重新训练一次 但这将进一步增加成本

91
00:09:48,370 --> 00:09:53,230
最后一种失败模式是错误预测的代价很高的情况

92
00:09:53,390 --> 00:09:57,750
你应该考虑的一件事是 每次你重新训练你的模型时 它都会引入风险

93
00:09:57,910 --> 00:10:02,840
这种风险来自于你用来训练模型的数据在某些方面可能是不好的

94
00:10:03,000 --> 00:10:09,120
它可能被破坏了 它可能被对手攻击了 或者它可能不再具有代表性了

95
00:10:09,180 --> 00:10:11,640
你的模型需要在所有情况下表现良好

96
00:10:11,800 --> 00:10:24,440
所以你再训练的频率越高 你对模型的失败就越敏感 你就越需要考虑我们如何确保我们仔细地评估这个模型 这样我们就不会过度地承担风险 再训练的风险太大

97
00:10:24,600 --> 00:10:28,040
通常，当你准备好从定期再培训中继续前进时，是时候开始迭代你的策略了。

98
00:10:28,480 --> 00:10:30,460
这是

99
00:10:30,500 --> 00:10:36,300
这节课我们会讲到一些工具这些工具可以帮助你了解如何在策略下进行迭代

100
00:10:36,970 --> 00:10:47,350
你不需要深入熟悉每一个工具 但我希望在这里给你一些提示 当你开始思考如何使你的模型更好时 你可以使用这些提示

101
00:10:47,510 --> 00:10:55,390
因此，本节的主要收获是，我们将使用监测和可观察性来确定我们想要对再培训策略做出哪些改变。

102
00:10:55,550 --> 00:11:04,100
我们将通过监控真正重要的指标 我们最关心的指标 然后使用它们所有的指标和信息进行调试来做到这一点

103
00:11:04,260 --> 00:11:08,180
当我们调试模型的问题时 这可能会导致对模型的重新训练

104
00:11:08,340 --> 00:11:18,600
但更广泛地说，我们可以把它看作是再训练策略的改变，就像改变我们的再训练触发器，改变你的离线测试，我们的抽样策略，我们用于可观察性的指标，等等。

105
00:11:18,760 --> 00:11:31,500
最后 迭代策略的另一个原则是 当你对你的监控更有信心时 当你对你的模型中出现的问题更有信心时 你就可以开始在你的系统中引入更多的自动化

106
00:11:31,660 --> 00:11:36,420
所以一开始要手动做 然后当你对自己的监控更有信心时 开始自动化

107
00:11:36,580 --> 00:11:41,500
让我们谈谈如何在生产中监控和调试模型 这样我们就可以弄清楚如何改进我们的再训练策略

108
00:11:41,660 --> 00:11:49,140
这里的TLDR就像这节课的很多部分一样，这里还没有真正的标准或最佳实践，还有很多不好的建议。

109
00:11:49,300 --> 00:11:55,700
我们在这里要遵循的主要原则是我们将重点关注那些真正重要的事情以及那些倾向于打破经验的事情，

110
00:11:55,800 --> 00:12:04,800
我们还会计算所有其他你可能听说过的信号，数据漂移，所有这些其他的东西，但我们主要会用那些来调试和观察。

111
00:12:04,970 --> 00:12:06,850
在生产环境中监视模型意味着什么

112
00:12:07,010 --> 00:12:15,650
我的想法是 你有一些度量来评估你的模型的质量 比如你的准确性 你有一个度量随时间变化的时间序列

113
00:12:15,810 --> 00:12:18,900
你要回答的问题是 这是坏事 还是好事

114
00:12:19,060 --> 00:12:22,780
我是否需要注意这种退化 或者我不需要注意

115
00:12:22,940 --> 00:12:27,560
所以我们需要回答的问题是 当我们进行监控时 我们应该关注哪些指标

116
00:12:27,950 --> 00:12:31,630
我们如何判断这些指标是否糟糕 是否需要干预

117
00:12:31,790 --> 00:12:34,950
最后 我们将讨论一些工具 这些工具可以帮助你完成这个过程

118
00:12:35,110 --> 00:12:38,190
选择正确的指标来监控可能是这个过程中最重要的部分

119
00:12:38,350 --> 00:12:45,190
这里是你可以看到的不同类型的参数或信号 如果你能够获得它们 它们将按照它们的价值排序

120
00:12:45,350 --> 00:12:50,170
你能看到的最有价值的东西是结果数据 或者来自用户的反馈

121
00:12:50,330 --> 00:12:54,990
如果你能接触到信号 那么这是目前最重要的事情

122
00:12:55,240 --> 00:13:00,760
不幸的是，没有放之四海而皆准的方法来做到这一点，因为这很大程度上取决于你正在构建的产品的具体情况。

123
00:13:00,920 --> 00:13:06,440
例如 如果你正在构建一个推荐系统 那么你可能会基于用户是否点击了推荐来衡量反馈

124
00:13:06,460 --> 00:13:11,870
但如果你正在制造一辆自动驾驶汽车，那就不是一个有用的甚至是可行的信号。

125
00:13:12,030 --> 00:13:18,080
相反，你可能会收集用户是否干预并抓住方向盘从汽车手中接管自动驾驶仪的数据。

126
00:13:18,240 --> 00:13:29,370
这实际上更像是一个产品设计或产品管理的问题你如何设计你的产品使你能够从用户那里获得反馈作为产品体验的一部分

127
00:13:29,530 --> 00:13:33,600
我们会回来，在机器学习产品管理讲座中更多地讨论这个。

128
00:13:33,760 --> 00:13:37,760
下一个最有价值的信号是模型性能指标 如果你能得到它的话

129
00:13:37,920 --> 00:13:40,760
这些是你的离线模型指标 比如准确性

130
00:13:40,920 --> 00:13:44,740
它不如用户反馈有用的原因在于失配

131
00:13:44,920 --> 00:13:55,810
所以我认为许多机器学习从业者都有一个共同的经历 比如说 你花了一个月的时间试图让你的准确率提高一到两个百分点 然后你部署了新版本的模型 结果你的用户并不关心

132
00:13:55,970 --> 00:14:01,010
他们的反应和以前一样 甚至更糟 对新的 理论上更好的模型版本

133
00:14:01,170 --> 00:14:04,290
至少在某种程度上 很少有借口不这样做

134
00:14:04,450 --> 00:14:07,390
你可以每天标记一些生产数据

135
00:14:07,600 --> 00:14:14,000
你可以设置一个随叫随到的轮换，或者每天开一个标签派对，和你的队友一起花30分钟，

136
00:14:14,000 --> 00:14:22,600
每个标记10或20个数据点，即使是很小的数量也会让你对模型的性能随时间的变化趋势有所了解。

137
00:14:22,820 --> 00:14:28,870
如果您不能度量实际的模型性能度量 那么下一个最好的选择就是代理度量

138
00:14:28,950 --> 00:14:32,970
代理指标是与糟糕的和所有的性能相关的指标

139
00:14:32,970 --> 00:14:35,390
这些大多是特定于领域的 所以如

140
00:14:35,550 --> 00:14:42,670
如果您正在使用语言模型构建文本生成 那么这里的两个示例将是重复输出和有害输出

141
00:14:42,930 --> 00:14:47,340
如果你正在构建一个推荐系统 那么个性化回复的共享就是一个例子

142
00:14:47,500 --> 00:14:52,460
如果您看到更少的个性化回复，那么这可能是您的模型正在做一些不好的事情。

143
00:14:52,620 --> 00:14:56,460
如果您正在寻找代理度量的想法，那么边缘情况可能是很好的代理度量。

144
00:14:56,620 --> 00:15:02,560
如果你知道你的模型存在某些问题 如果这些问题越来越普遍 那么这可能意味着你的模型做得不好

145
00:15:02,720 --> 00:15:04,760
这就是今天代理度量的实际方面

146
00:15:04,920 --> 00:15:08,440
它们是特定领域的 你要么有好的代理指标 要么没有

147
00:15:08,600 --> 00:15:09,920
但我不认为一定要这样

148
00:15:10,080 --> 00:15:18,760
有一个我很感兴趣的学术方向 它的目标是能够采取任何你关心的度量 比如你的准确性 并在以前看不见的数据上近似它

149
00:15:18,920 --> 00:15:25,560
那么 我们认为我们的模型在这些新数据上做得如何 这将使这些代理指标在实际中更有用

150
00:15:25,720 --> 00:15:37,790
这里有许多不同的方法，从训练辅助模型来预测您的主模型在离线数据上的表现，到启发式方法，再到人工循环方法。

151
00:15:37,950 --> 00:15:38,590
所以值得一试。

152
00:15:38,950 --> 00:15:42,490
如果你对未来两三年人们会怎么做感兴趣的话。

153
00:15:42,830 --> 00:15:45,350
然而，这些文献中有一个不幸的结果值得指出，

154
00:15:45,550 --> 00:15:56,220
可能不可能有一种方法可以在所有情况下使用来近似你的模型在分布外数据上的表现。

155
00:15:56,380 --> 00:16:05,480
考虑这个问题的方法是 假设你在看输入数据来预测模型在这些输入点上的表现 然后标签分布发生了变化

156
00:16:05,640 --> 00:16:12,460
如果你只关注输入点 那么你如何能够在你的近似度量中考虑到标签分布的变化

157
00:16:12,620 --> 00:16:14,980
但这个结果也有更多的理论四舍五入

158
00:16:15,140 --> 00:16:17,700
好了 回到我们更实用的日程安排

159
00:16:17,860 --> 00:16:19,940
你可以看的下一个信号是数据质量

160
00:16:19,960 --> 00:16:24,500
数据质量测试只是一套规则 你可以应用它来衡量你的数据质量

161
00:16:24,660 --> 00:16:28,260
这是在处理这样的问题 数据在多大程度上反映了现实

162
00:16:28,420 --> 00:16:32,500
它有多全面 随着时间的推移 它有多一致

163
00:16:32,660 --> 00:16:44,020
数据质量测试的一些常见示例包括检查数据是否具有正确的模式 每个列中的值是否在您期望的范围内 以及您有足够的列 没有太多丢失的数据

164
00:16:44,180 --> 00:16:51,710
简单的规则 比如这个有用的原因是因为数据问题往往是机器学习模型在实践中最常见的问题

165
00:16:52,110 --> 00:16:59,760
这是谷歌的一份报告他们用一个特定的机器学习模型涵盖了15年不同的管道中断

166
00:16:59,920 --> 00:17:05,920
他们的主要发现是 该模型发生的大多数中断与机器学习根本没有多大关系

167
00:17:06,080 --> 00:17:10,350
它们通常是分布式系统问题 或者通常是数据问题

168
00:17:10,510 --> 00:17:19,980
他们给出的一个例子是一种常见的故障类型，其中数据管道失去了读取它所依赖的数据源的权限，因此开始出现故障。

169
00:17:20,140 --> 00:17:24,980
因此 这些类型的数据问题通常会导致模型在生产中严重失败

170
00:17:25,140 --> 00:17:28,080
下一个最有用的信号是分布漂移

171
00:17:28,240 --> 00:17:36,920
尽管分布漂移是一个不如用户反馈有用的信号 但能够测量数据分布是否发生变化仍然非常重要

172
00:17:37,080 --> 00:17:47,390
为什么呢 好吧 你的模型的性能只有在它被评估的数据是从与它被训练的相同的分布中抽样时才能得到保证 这在实践中会产生巨大的影响

173
00:17:47,550 --> 00:18:03,260
最近的例子包括大流行期间模型行为的全面变化，因为像Corona这样的词出现了新的会议或漏洞和再培训管道，给公司造成了数百万美元的损失，因为它们导致了数据分布的变化。

174
00:18:03,520 --> 00:18:05,640
分布漂移在野外以不同的方式表现出来

175
00:18:05,800 --> 00:18:07,280
你可能会看到几种不同的类型

176
00:18:07,440 --> 00:18:25,080
所以你可能会有一个瞬时漂移，比如当一个模型被部署到一个新的领域，或者一个错误被引入到预处理管道中，或者一些大的外部变化，比如covid，你可能会有一个渐进的漂移，比如用户偏好随着时间的推移而改变，或者新的概念不断被添加到你的语料库中。

177
00:18:25,240 --> 00:18:33,590
您可以有周期性的漂移，比如如果您的用户偏好是季节性的，或者您可以有临时的漂移，比如如果恶意用户攻击您的模型。

178
00:18:33,750 --> 00:18:37,730
每一种不同类型的漂流可能需要用稍微不同的方法来检测

179
00:18:38,050 --> 00:18:39,810
那么 你如何判断你的分布是否存在漂移呢

180
00:18:39,970 --> 00:18:45,250
这里我们要采取的方法是首先我们要选择一个好的数据窗口作为下一步的参考

181
00:18:45,410 --> 00:18:46,830
如何选择该引用

182
00:18:46,990 --> 00:18:56,110
你可以使用一个你认为是健康的生产数据的固定窗口 所以如果你认为你的模型在月初是健康的 你可以把它作为你的参考窗口

183
00:18:56,270 --> 00:19:07,680
一些论文主张滑动生产数据窗口作为参考 但在实践中 大多数时候 大多数人做的是 他们会使用他们的验证数据作为参考

184
00:19:07,840 --> 00:19:14,650
一旦你有了这些参考数据 你就可以选择新的生产数据窗口来衡量你的分销距离

185
00:19:14,810 --> 00:19:21,790
对于如何选择测量漂移的数据窗口 并没有一个真正的超级原则方法 它往往是非常特定于问题的

186
00:19:22,080 --> 00:19:33,460
一个实用的解决方案是 很多人只选择一个窗口大小 或者只选择几个窗口大小有一些合理的数据量 这样不会太混乱 然后滑动这些窗口

187
00:19:33,620 --> 00:19:40,420
最后 一旦有了参考窗口和生产窗口 就可以使用分布距离度量来比较这两个窗口

188
00:19:40,580 --> 00:19:42,000
那么应该使用什么度量标准呢

189
00:19:42,200 --> 00:19:53,500
让我们从一维的情况开始，你有一个特定的一维特征，你可以计算这个特征在参考窗口和生产窗口上的密度，

190
00:19:53,500 --> 00:20:01,600
考虑这个问题的方法是你要有一个度规来近似这两个分布之间的距离。

191
00:20:01,840 --> 00:20:07,340
这里有几个选择。通常推荐的是kl散度和ks测试。

192
00:20:07,360 --> 00:20:10,540
不幸的是 这些通常被推荐 但它们也是糟糕的选择

193
00:20:10,700 --> 00:20:20,170
有时更好的选择是使用无穷范数或一范数，这是谷歌提倡使用的，或者是推土机的距离，这更像是统计学上的主要方法。

194
00:20:20,330 --> 00:20:28,620
我不打算在这里详细介绍这些指标 但如果你想了解更多关于为什么通常推荐的指标不那么好而其他指标更好的原因 请查看底部的博客文章

195
00:20:28,780 --> 00:20:29,940
这是一维的情况。

196
00:20:29,960 --> 00:20:33,900
如果你只有一个输入特征你想测量它的分布距离

197
00:20:34,060 --> 00:20:40,140
但在现实世界中 对于大多数模型 我们可能有很多输入特征 甚至是非结构化的高维数据

198
00:20:40,300 --> 00:20:43,400
那么在这些情况下 我们如何处理探测分布演练呢

199
00:20:43,700 --> 00:20:48,400
你可以考虑做的一件事是单独测量所有特征上的漂移。

200
00:20:48,580 --> 00:20:53,580
你会遇到的问题是如果你有很多特征 你就会遇到多重假设检验问题

201
00:20:53,740 --> 00:20:55,820
其次 这没有捕捉到相互关系

202
00:20:55,980 --> 00:21:06,280
所以如果你有两个特征，每个特征的分布保持不变，但特征之间的相关性改变了，那么用这种系统是无法捕捉到的。

203
00:21:06,460 --> 00:21:10,380
另一种常见的做法是只在最重要的特征上测量漂移

204
00:21:10,540 --> 00:21:16,990
这里的一个启发是，一般来说，测量模型输出上的漂移比测量输入上的漂移更有用。

205
00:21:17,150 --> 00:21:19,470
原因是输入一直在变化

206
00:21:19,630 --> 00:21:24,050
你的模型在一定程度上对输入的分布移位是稳健的

207
00:21:24,210 --> 00:21:27,370
但如果输出发生变化 那可能更表明存在问题

208
00:21:27,530 --> 00:21:32,730
而且 对于大多数机器学习模型来说 输出往往是低维的 所以更容易监控

209
00:21:32,890 --> 00:21:38,540
你还可以对输入特征的重要性进行排序 并测量最重要特征的漂移

210
00:21:38,700 --> 00:21:47,120
你可以用启发式的方法来做 用那些你认为重要的 或者你可以计算一些特征重要性的概念 然后用它来给你想要监控的特征排序

211
00:21:47,280 --> 00:21:54,440
最后 还有一些指标可以用来计算或近似高维分布之间的分布距离

212
00:21:54,600 --> 00:22:00,670
最值得研究的两个是最大平均差值和近地移动距离。

213
00:22:01,050 --> 00:22:12,840
这里需要注意的是，这些很难解释，如果你触发了最大均值差异警报，这并不能告诉你在哪里寻找导致分布漂移的潜在故障。

214
00:22:13,000 --> 00:22:17,870
在我看来，一个更有原则的方法是使用投影来测量模型的高维输入的分布漂移。

215
00:22:18,030 --> 00:22:31,180
投影的概念是你给模型取一些高维的输入，或者输出一个图像或文本，或者只是一个非常大的特征向量，然后你通过一个函数运行它。

216
00:22:31,340 --> 00:22:35,930
所以你的模型做出预测的每个数据点都会被这个投影函数标记。

217
00:22:36,090 --> 00:22:40,430
投影函数的目标是降低输入的维数。

218
00:22:40,770 --> 00:22:48,530
一旦你降低了维度 你就可以对高维数据的低维表示进行漂移检测

219
00:22:48,690 --> 00:22:58,570
这个方法的伟大之处在于它适用于任何类型的数据 无论是图像还是文本或其他 无论维度是什么或数据类型是什么

220
00:22:58,730 --> 00:23:02,450
而且它非常灵活 有许多不同类型的投影都是有用的

221
00:23:02,610 --> 00:23:08,030
你可以定义分析投影 它只是输入数据的函数

222
00:23:08,190 --> 00:23:17,000
这些东西就像看图像的平均像素值 或者一个句子的长度作为模型的输入 或者任何你能想到的函数

223
00:23:17,160 --> 00:23:23,260
分析预测是高度可定制的 它们是高度可解释性的 并且经常可以在实践中发现问题

224
00:23:23,400 --> 00:23:28,600
如果你不想用你的领域知识通过写分析函数来制作投影，那么你也可以做一般的投影，

225
00:23:28,600 --> 00:23:38,500
比如随机投影，或者统计投影，比如通过自动编码器运行你的每个输入，诸如此类。

226
00:23:38,720 --> 00:23:43,190
这是我对检测高维和非结构化数据漂移的建议

227
00:23:43,300 --> 00:23:51,700
同样值得注意的是这个投影的概念，因为我们会在讨论持续学习的其他方面时看到这个概念在其他一些地方出现，

228
00:23:51,700 --> 00:24:00,000
当您监视模型时，这种分布漂移是一个重要的信号，事实上，这是许多人在考虑模型监视时所想到的。

229
00:24:00,200 --> 00:24:01,640
那么为什么我们把它排在这么低的位置呢?

230
00:24:01,800 --> 00:24:04,080
让我们来谈谈观察这种分布漂移的缺点

231
00:24:04,240 --> 00:24:10,160
我认为最大的问题是 模型被设计成在一定程度的分布漂移上是健壮的

232
00:24:10,500 --> 00:24:14,300
左边的图展示了一个玩具样例来证明这一点，我们有一个分类器，它被训练来预测两个类别，

233
00:24:14,300 --> 00:24:27,200
我们推导出了一个综合分布移位，将这些点从左上方的红色点移到右下方的红色点。

234
00:24:27,570 --> 00:24:29,490
这两种分布非常不同

235
00:24:29,650 --> 00:24:36,780
底部图表和右侧图表中的边际分布分布之间的距离非常大

236
00:24:36,960 --> 00:24:45,920
但实际上，模型在训练数据上的表现与在生产数据上的表现一样好，因为移位只是直接沿着分类器边界移位。

237
00:24:46,000 --> 00:24:47,600
所以这是一个玩具的例子，它表明，你知道，当我们监控我们的模型时，分布变化并不是我们真正关心的事情，

238
00:24:47,600 --> 00:24:58,500
因为只知道分布变化并不能告诉我们模型对分布变化的反应。

239
00:24:58,660 --> 00:25:09,530
另一个值得说明的例子是我在研究生院时做的一些研究 用物理模拟器生成的数据来解决现实世界机器人的问题

240
00:25:09,690 --> 00:25:18,330
对于我们关心的测试用例，我们使用的数据是高度非分布的，数据看起来像这些保真度随机图像，像左边。

241
00:25:18,490 --> 00:25:26,650
我们发现 通过对大量低保真随机图像进行训练 我们的模型实际上能够推广到现实世界的场景 比如右边的这个

242
00:25:26,810 --> 00:25:32,050
因此，在模型被训练的数据和被评估的数据之间，巨大的分布会直观地发生变化。

243
00:25:32,210 --> 00:25:36,850
但在测量分布漂移的理论限制之外 它在这两方面的表现都很好

244
00:25:37,010 --> 00:25:38,450
这在实践中也很难做到

245
00:25:38,610 --> 00:25:40,770
你必须正确选择窗口大小

246
00:25:40,930 --> 00:25:42,930
你必须保存所有这些数据

247
00:25:43,090 --> 00:25:48,720
你需要选择指标 您需要定义投影以使数据的维度更低

248
00:25:48,880 --> 00:25:51,360
所以这不是一个非常可靠的信号

249
00:25:51,520 --> 00:25:56,810
这就是为什么我们提倡一次性关注那些与真正重要的事情更相关的东西

250
00:25:56,970 --> 00:26:07,780
你应该考虑的最后一件事是你的标准系统指标 如CPU利用率或你的模型占用了多少GPU内存 诸如此类的事情

251
00:26:07,990 --> 00:26:13,570
这些并不能告诉你模型的实际运行情况 但它们可以告诉你什么时候出了问题

252
00:26:13,750 --> 00:26:19,620
好的 这是所有不同类型的指标或信号的排序如果你能计算的话

253
00:26:19,840 --> 00:26:26,800
但是为了给你们一个更具体的建议 我们还要讨论一下在实际中计算这些不同的信号有多难

254
00:26:26,960 --> 00:26:32,260
我们将把每种信号的值分别放在y轴和x轴上

255
00:26:32,420 --> 00:26:36,080
我们将讨论可行性 比如 测量这些东西有多容易

256
00:26:36,300 --> 00:26:44,440
衡量结果或反馈有很大的可变性 因为它的可行性很大程度上取决于你的产品是如何建立的 而不是你正在处理的问题的类型

257
00:26:44,600 --> 00:26:49,920
度量模型性能往往是最不可行的事情，因为它涉及到收集一些标签。

258
00:26:50,080 --> 00:26:54,600
所以像代理指标这样的东西更容易计算因为它们不涉及标签

259
00:26:54,760 --> 00:27:06,220
然而，系统度量和数据质量度量是非常可行的，因为你可以使用现成的库和工具，而且从机器学习的角度来看，它们不需要做任何特殊的事情。

260
00:27:06,300 --> 00:27:08,400
因此，这里的实用建议是，进行基本的数据质量检查实际上是零遗憾的，特别是如果你处于经常重新训练模型的阶段，

261
00:27:08,400 --> 00:27:23,700
因为在实践中，数据质量问题是导致模型性能不佳的最常见原因之一，而且它们非常容易实现。

262
00:27:23,700 --> 00:27:30,000
下一个建议是找到一些衡量反馈或模型性能的方法，如果你真的不能做到这两点，那就使用代理度量。

263
00:27:30,100 --> 00:27:45,100
即使这种衡量模型性能的方法很粗糙，或者不可扩展，这也是最重要的信号，也是唯一能够可靠地告诉你模型是否在做它应该做的事情。

264
00:27:45,100 --> 00:27:57,300
如果你的模型产生的是低维输出 比如你在做二分类或者类似的事情 那么监控输出分布 分数分布 也会很有用 很容易做到

265
00:27:57,500 --> 00:28:02,800
最后，当你发展你的系统时，就像一旦你有了这些基础，你在你的模型上迭代，你对评估越来越有信心，

266
00:28:02,800 --> 00:28:12,200
我鼓励你们采用一种思维方式，从可观察性的概念中借鉴你所计算的指标。

267
00:28:12,420 --> 00:28:17,920
那么 当你认为监控是测量已知的未知时 可观察性思维是什么

268
00:28:18,100 --> 00:28:31,000
因此 如果我们知道我们关心的是4个 5个或10个指标 比如准确性 延迟 用户反馈 监控方法就是测量每一个信号

269
00:28:31,160 --> 00:28:33,800
我们甚至可以针对其中几个关键参数设置警报

270
00:28:33,960 --> 00:28:37,920
另一方面 可观测性是关于测量未知的未知

271
00:28:38,080 --> 00:28:48,590
它是关于在系统崩溃时能够提出任意问题的能力 例如 我的准确性如何在我一直在考虑的所有不同区域中爆发

272
00:28:48,930 --> 00:28:52,430
我的每个特性的分布漂移是什么

273
00:28:52,770 --> 00:29:00,110
而不是你必须设置警报的信号 因为你没有任何理由相信这些信号会在未来造成问题

274
00:29:00,280 --> 00:29:05,120
但是当你在调试的时候，能够看到这些东西是很有帮助的。

275
00:29:05,280 --> 00:29:16,090
如果你选择采用观察者早期思维 我强烈建议你这样做 尤其是在机器学习中 因为能够回答任意问题来调试模型是非常非常关键的

276
00:29:16,250 --> 00:29:31,930
还有一些暗示。首先，你应该围绕上下文，或者是构成你正在计算的度量标准的原始数据，因为你想要能够一直钻到构成度量标准已经退化的潜在数据点本身。

277
00:29:32,090 --> 00:29:36,790
顺便说一句 保留原始数据对再训练之类的事情也很有帮助

278
00:29:36,980 --> 00:29:39,740
第二个暗示是 你可以对测量有点疯狂

279
00:29:39,900 --> 00:29:46,300
你可以为任何你能想到的未来可能出错的事情定义很多不同的指标，但你不应该对每一个都设置警报，

280
00:29:46,300 --> 00:29:55,000
或者至少不是非常精确的警报，因为你不想有收到太多警报的问题。

281
00:29:55,220 --> 00:29:58,940
当出现问题时 您希望能够使用这些信号进行调试

282
00:29:59,100 --> 00:30:00,380
Drift 就是一个很好的例子

283
00:30:00,540 --> 00:30:06,230
这对于调试非常有用 因为 假设你昨天的准确度低于这个月的其他时间

284
00:30:06,390 --> 00:30:15,670
你可能调试的一种方法是 试着看看是否有任何输入字段或投影看起来不同 将昨天和这个月的其他时间区分开来

285
00:30:15,830 --> 00:30:18,430
这些可能是你的模型出了什么问题的指示器

286
00:30:18,450 --> 00:30:24,610
我对模型监控和可观察性的最后一条建议是，超越总体指标是非常重要的。

287
00:30:24,810 --> 00:30:33,650
假设你的模型有99%的准确率 假设这很好 但对于一个特定的用户他恰好是你最重要的用户 它只有50%的准确率

288
00:30:33,810 --> 00:30:35,930
我们真的还能认为模型是好的吗

289
00:30:36,090 --> 00:30:43,230
所以处理这个问题的方法是通过标记重要的子组或数据队列，并能够沿着这些队列切片和切片的性能，

290
00:30:43,530 --> 00:30:46,410
甚至可能对这些人设置警报。

291
00:30:46,500 --> 00:30:50,500
这方面的一些例子是你不想让你的模型有偏见的用户类别，

292
00:30:50,600 --> 00:30:56,700
或者对你的业务特别重要的用户类别，或者只是你可能希望你的模型在他们身上表现不同的用户类别，

293
00:30:56,800 --> 00:31:06,700
比如，如果你在很多不同的地区或不同的语言中推出，看看你的表现在这些地区或语言之间是如何突破的可能会有所帮助。

294
00:31:07,080 --> 00:31:11,990
好了 这是对不同指标的深入研究 你可以为了监控的目的来研究这些指标

295
00:31:12,150 --> 00:31:15,750
我们要讨论的下一个问题是如何判断这些指标是好是坏

296
00:31:15,910 --> 00:31:18,530
有几个不同的选项 您会看到推荐的

297
00:31:18,950 --> 00:31:25,570
一个我不推荐的，我之前提到过一点，是两个样本统计测试，比如AKS测试。

298
00:31:25,600 --> 00:31:30,200
我不推荐这样做的原因是，如果你想想这两个样本测试实际上在做什么，

299
00:31:30,200 --> 00:31:38,300
它们试图返回一个p值表示这个数据和这个数据不来自同一分布的可能性。

300
00:31:38,510 --> 00:31:45,670
当你有很多数据时，这意味着即使分布中很小的变化也会得到非常小的p值。

301
00:31:46,050 --> 00:31:52,850
因为即使分布只有一点点不同 如果你有很多样本 你可以很自信地说这些是不同的分布

302
00:31:53,010 --> 00:31:54,450
但这不是我们真正关心的

303
00:31:54,610 --> 00:31:57,970
因为模型对少量的分布变化具有鲁棒性

304
00:31:58,130 --> 00:32:00,710
比统计测试更好的选择包括以下几种

305
00:32:01,020 --> 00:32:04,620
您可以有固定的规则，比如在这个列中不应该有任何空值。

306
00:32:04,780 --> 00:32:10,510
你可以有特定的范围 所以你的准确率应该总是在90%到95%之间

307
00:32:10,670 --> 00:32:16,750
可以有预测的范围，所以准确度在一个现成的异常检测器认为合理的范围内。

308
00:32:16,910 --> 00:32:21,470
或者在这个信号中也有对新模式的无监督检测

309
00:32:21,630 --> 00:32:26,490
实践中最常用的是前两个固定规则和指定范围。

310
00:32:26,740 --> 00:32:31,820
但通过异常检测预测的范围也非常有用，特别是如果你的数据中有一些季节性。

311
00:32:31,980 --> 00:32:37,750
关于模型监视 我想讨论的最后一个主题是可用于监视模型的不同工具

312
00:32:37,910 --> 00:32:39,670
第一类是系统监视工具

313
00:32:39,830 --> 00:32:42,550
这是一个相当成熟的领域 有很多不同的公司

314
00:32:42,710 --> 00:32:48,250
这些工具可以帮助你检测任何软件系统的问题，而不仅仅是机器学习模型。

315
00:32:48,470 --> 00:32:51,670
当出现问题时 它们提供了设置警报的功能

316
00:32:51,830 --> 00:33:00,600
大多数云提供商在这方面都有相当不错的解决方案，但如果你想要更好的解决方案，你可以看看其中一个可观察性或监控特定工具，如Honeycomb或Data Dog。

317
00:33:00,760 --> 00:33:03,220
你几乎可以监控这些系统中的任何东西

318
00:33:03,400 --> 00:33:09,480
这就提出了一个问题 我们是否也应该使用这样的系统来监控机器学习指标

319
00:33:09,640 --> 00:33:15,950
如果你有兴趣了解为什么这是可行的，我建议你阅读一篇关于这个话题的很棒的博客文章。

320
00:33:16,110 --> 00:33:20,030
但是做起来很痛苦，所以也许用一些ml特定的东西更好。

321
00:33:20,050 --> 00:33:23,570
这里 就ML特定的工具而言 有一些开源工具

322
00:33:23,810 --> 00:33:26,730
最流行的两个显然是AI和ylog

323
00:33:26,890 --> 00:33:36,410
它们的相似之处在于，你向它们提供数据样本，它们会生成一份漂亮的报告，告诉你它们的分布如何变化，你的模型指标如何变化，等等。

324
00:33:36,570 --> 00:33:41,010
这些工具的最大限制是它们不能为您解决数据基础设施和规模问题

325
00:33:41,170 --> 00:33:45,010
你仍然需要能够把所有的数据放到一个地方 这样你就可以用这些工具来分析它

326
00:33:45,170 --> 00:33:48,730
实际上 这是这个问题最难的部分之一

327
00:33:48,800 --> 00:33:59,800
这些工具之间的主要区别在于，为什么日志更侧重于从边缘收集数据，而它们的方法是通过在推理时间本身将数据聚合成统计概况，

328
00:33:59,800 --> 00:34:07,800
因此，您不需要将所有数据从推理设备传输回您的云，这在某些情况下可能非常有用。

329
00:34:08,010 --> 00:34:13,290
最后，有很多不同的sass供应商提供ML监控和可观察性。

330
00:34:13,450 --> 00:34:18,070
我的创业公司 龙门架有一些功能 还有很多其他的选择

331
00:34:18,200 --> 00:34:27,000
好了，我们已经讨论了模型监控和可观察性，在持续学习的背景下，监控和可观察性的目标是

332
00:34:27,000 --> 00:34:35,700
给你信号，告诉你需要弄清楚你的持续学习系统出了什么问题，以及你如何改变策略来影响结果。

333
00:34:35,880 --> 00:34:47,540
接下来，我们将讨论持续学习循环中的每个阶段，有哪些不同的方法可以超越基础知识，利用我们从监测和可观察性中学到的知识来改进这些阶段。

334
00:34:47,840 --> 00:34:49,720
持续学习循环的第一阶段是记录

335
00:34:49,880 --> 00:34:53,880
提醒一下 日志记录的目标是将数据从模型中获取到可以对其进行分析的位置

336
00:34:54,040 --> 00:34:57,040
要回答的关键问题是 我应该记录哪些数据

337
00:34:57,200 --> 00:34:59,940
对我们大多数人来说 最好的答案就是记录你所有的数据

338
00:35:00,260 --> 00:35:03,180
存储很便宜 有数据总比没有数据好

339
00:35:03,340 --> 00:35:05,780
但是在某些情况下你不能这样做 例如

340
00:35:05,900 --> 00:35:09,400
如果你的模型有太多的流量，以至于记录所有流量的成本太高，如果你有数据隐私问题，如果你不允许查看用户的数据，

341
00:35:09,400 --> 00:35:25,500
或者如果你在边缘运行你的模型，取回所有数据太昂贵了因为你没有足够的网络带宽，如果你不能记录所有数据，你可以做两件事。

342
00:35:25,600 --> 00:35:34,000
第一个是分析分析的想法是，而不是把所有的数据发送回你的云，然后用它来进行监控，观察或再培训，

343
00:35:34,000 --> 00:35:42,400
相反，您可以计算边缘数据的统计配置文件，以描述您所看到的数据分布。

344
00:35:42,630 --> 00:35:48,150
这样做的好处是 从数据安全的角度来看 它很好 因为它不需要你把所有的数据发送回家

345
00:35:48,300 --> 00:35:52,600
它将您的存储成本降至最低。最后，你不会错过故事中发生的事情，

346
00:35:52,600 --> 00:36:00,000
这是下一种方法要讨论的问题，这种方法主要用于安全关键型应用程序。

347
00:36:00,190 --> 00:36:04,530
另一种方法是抽样。在抽样中，你只需要取一些数据点，然后把它们寄回家。

348
00:36:04,690 --> 00:36:09,130
抽样的优点是它对推理资源的影响最小

349
00:36:09,290 --> 00:36:16,810
所以你不需要实际花费计算预算来计算概要 你可以访问原始数据来调试和再训练

350
00:36:16,970 --> 00:36:24,450
这是我们在其他应用中推荐的做法，应该更详细地描述统计概要文件是如何工作的，因为这很有趣。

351
00:36:24,610 --> 00:36:36,840
假设你有一个数据流来自两类 猫和狗 你想在不查看所有原始数据的情况下估计猫和狗随时间的分布

352
00:36:37,000 --> 00:36:40,900
例如，也许在过去你看到了三个关于狗的例子和两个关于猫的例子。

353
00:36:41,440 --> 00:36:46,000
可以存储的统计配置文件将这些数据总结为直方图

354
00:36:46,340 --> 00:36:49,470
直方图显示我们看到了三个狗和两个猫的例子

355
00:36:49,630 --> 00:37:00,280
随着时间的推移 随着越来越多的例子涌入 而不是实际存储这些数据 我们可以增加直方图并跟踪我们在一段时间内看到的每个类别的总例子数量

356
00:37:00,400 --> 00:37:08,500
一个简单的统计事实是，对于很多你可能感兴趣的统计数据，分位数意味着准确性，

357
00:37:08,600 --> 00:37:19,200
你可以计算的其他统计数据，你可以非常准确地近似这些统计数据通过使用最小尺寸的统计轮廓图。

358
00:37:19,400 --> 00:37:24,640
所以，如果你有兴趣去切题，了解更多关于计算机科学中有趣的话题，我建议你去看看。

359
00:37:24,800 --> 00:37:27,240
持续学习循环的下一步是管理

360
00:37:27,400 --> 00:37:35,400
提醒你一下，管理的目标是把你无限的生产数据流，这些数据可能是未标记的，然后把它变成一个有限的数据库，

361
00:37:35,400 --> 00:37:40,700
它有它需要的所有丰富内容，比如训练你的模型的标签。

362
00:37:40,920 --> 00:37:48,760
我们在这里需要回答的关键问题与我们在对数时间采样数据时需要回答的问题类似，也就是，我们应该选择哪些数据进行富集?

363
00:37:48,760 --> 00:37:52,060
最基本的方法就是随机取样

364
00:37:52,220 --> 00:37:58,420
但是 特别是当您的模型变得更好时 您在生产中看到的大多数数据实际上可能对改进您的模型没有多大帮助

365
00:37:58,580 --> 00:38:00,800
如果你这样做，你可能会错过一些罕见的课程或活动。

366
00:38:01,110 --> 00:38:12,760
比如你有一个事件在生产中每10000个例子中只发生一次，但你想在此基础上改进你的模型，那么如果你只是随机抽样，你可能根本不会对任何例子进行抽样。

367
00:38:12,920 --> 00:38:16,320
一种改进随机抽样的方法是分层抽样

368
00:38:16,480 --> 00:38:21,160
这里的想法是从不同的亚群体中采样特定比例的数据点

369
00:38:21,320 --> 00:38:33,380
所以在机器学习中 你可能对抽样进行分层的常见方法可能是抽样来获得类之间的平衡 或者抽样来获得你不想让你的模型有偏见的类别之间的平衡 比如性别

370
00:38:33,500 --> 00:38:46,200
最后，挑选数据来丰富的最先进和有趣的策略是，为了改进而挑选有趣的数据点，有几种不同的方法

371
00:38:46,200 --> 00:38:53,600
我们要讲的第一点是让有趣的数据由你的用户驱动，这将来自用户反馈和反馈循环。

372
00:38:53,800 --> 00:38:59,280
第二种方法是通过定义错误情况或边缘情况来确定什么是自己感兴趣的数据

373
00:38:59,440 --> 00:39:02,360
第三步是用算法来定义。

374
00:39:02,520 --> 00:39:04,840
这是一种被称为主动学习的技术

375
00:39:05,000 --> 00:39:11,800
如果你已经有了一个反馈回路，或者在你的机器学习系统中有了一种收集用户反馈的方法，你真的应该这样做，

376
00:39:11,800 --> 00:39:18,800
如果你可以，那么这可能是最简单的，也是最有效的方式来挑选有趣的数据进行管理。

377
00:39:19,030 --> 00:39:24,550
它的工作方式是 你会根据用户不喜欢你的预测的信号来选择数据

378
00:39:24,710 --> 00:39:27,910
所以这可能是在与你的模型交互后被抛弃的用户

379
00:39:28,070 --> 00:39:31,750
这可能是他们对模型做出的特定预测提出了支持票

380
00:39:31,900 --> 00:39:38,800
可能是他们点击了你放在产品上的拇指向下的按钮，他们改变了你的模型为他们制作的标签，

381
00:39:38,800 --> 00:39:43,500
或者他们用自动系统进行干预，比如他们抓住自动驾驶系统的方向盘。

382
00:39:43,700 --> 00:39:49,700
如果你没有用户反馈，或者你需要更多的方法从系统中收集有趣的数据，

383
00:39:49,700 --> 00:39:54,600
第二种最有效的方法就是进行人工错误分析。

384
00:39:54,850 --> 00:39:57,950
它的工作方式是 我们将观察模型所犯的错误

385
00:39:58,100 --> 00:40:05,900
我们将对我们看到的不同类型的故障模式进行推理，我们将尝试编写帮助捕获这些错误模式的函数或规则，

386
00:40:05,900 --> 00:40:10,700
然后我们会用这些函数来收集更多的数据来表示那些错误情况。

387
00:40:10,990 --> 00:40:12,730
如何做到这一点有两个子类

388
00:40:13,000 --> 00:40:16,000
一种是我称之为基于相似性的管理

389
00:40:16,100 --> 00:40:26,300
它的工作方式是，如果你有一些数据代表你的错误，或者你认为可能是错误的数据，那么你可以选择一个单独的数据点或几个数据点，

390
00:40:26,300 --> 00:40:35,300
然后运行最近邻相似度搜索算法在你的数据流中找到最接近你的模型可能出错的数据点。

391
00:40:35,300 --> 00:40:42,200
第二种方法，可能更强大，但更难做到，叫做基于投影的策展。

392
00:40:42,400 --> 00:40:51,300
它的工作方式是，我们不只是选取一个例子然后选取它最近的邻居，相反，我们要找到一个错误情况，比如右下角的那个，

393
00:40:51,300 --> 00:40:57,900
比如有一个人骑着自行车过马路，然后我们要写一个函数来检测这种错误情况。

394
00:40:58,270 --> 00:41:03,960
这可以只是一个简单的神经网络 也可以只是写一些启发式的东西

395
00:41:04,120 --> 00:41:07,510
基于相似度的管理的优势在于它非常简单和快速，对吧?

396
00:41:07,680 --> 00:41:12,620
你只需要点击几个例子你就能得到和这些例子相似的东西。

397
00:41:12,780 --> 00:41:15,750
这已开始在实践中得到广泛应用。

398
00:41:15,940 --> 00:41:21,980
多亏了市场上矢量搜索数据库的爆炸式增长，这样做相对容易。

399
00:41:22,140 --> 00:41:25,220
这种方法特别适用于罕见的事件

400
00:41:25,380 --> 00:41:28,780
它们在你的数据集中不经常出现 但它们很容易被发现

401
00:41:28,900 --> 00:41:33,200
比如，如果你的自动驾驶汽车遇到了问题，你看到美洲驼过马路，

402
00:41:33,200 --> 00:41:39,000
基于相似性搜索的算法可能会在检测训练集中的其他美洲驼方面做得相当好。

403
00:41:39,120 --> 00:41:48,100
另一方面 基于投影的管理需要一些领域知识 因为它需要你更多地思考你在这里看到的特定错误情况是什么 并编写一个函数来检测它

404
00:41:48,260 --> 00:41:53,770
但它对更细微的误差模式有好处，在这种情况下，类似的研究算法可能筛选得太粗糙。

405
00:41:53,930 --> 00:42:01,210
它可能会找到表面上看起来与您正在检测的示例相似的示例，但实际上不会导致模型失败。

406
00:42:01,370 --> 00:42:07,090
最后一种整理数据的方法是自动完成 使用一类称为主动学习的算法

407
00:42:07,250 --> 00:42:18,570
主动学习的工作方式是 给定大量不可预测的数据 我们要做的是确定哪些数据点能最大程度地提高模型的性能 如果你接下来要标记这些数据点并对它们进行训练

408
00:42:18,730 --> 00:42:30,770
这些算法的工作方式是通过定义一个抽样策略或查询策略 然后你用一个定义了该策略的评分函数对所有的不合格样本进行排名 然后把得分最高的那些样本送去标记

409
00:42:30,930 --> 00:42:34,210
我将给你们快速介绍一些不同类型的评分函数

410
00:42:34,230 --> 00:42:42,210
如果你想了解更多这方面的知识 我推荐你看下面链接的博客文章 你有评分函数样本数据点模型对这些数据点非常不自信

411
00:42:42,370 --> 00:42:50,570
你有打分函数是通过尝试预测模型在这个数据点上的误差来定义的如果我们有一个标签

412
00:42:50,730 --> 00:42:55,690
你有评分函数 它们被设计用来检测那些看起来与你已经训练过的数据完全不同的数据

413
00:42:55,850 --> 00:42:58,850
那么我们能把这些数据点和我们的训练数据区分开来吗

414
00:42:59,010 --> 00:43:01,990
如果是这样 也许这些是我们应该取样和标签的

415
00:43:02,150 --> 00:43:11,030
我们有评分函数 它的设计目的是获取大量的数据点 并将其归结为最能代表该分布的少数数据点

416
00:43:11,190 --> 00:43:17,290
最后 还有一些评分函数是用来检测数据点的 如果我们在这些数据点上进行训练 我们认为我们会对训练产生很大的影响

417
00:43:17,450 --> 00:43:22,010
所以它们会有很大的期望梯度 或者会导致模型改变主意

418
00:43:22,170 --> 00:43:26,190
这只是你可能实现的不同类型的评分函数的快速浏览

419
00:43:26,610 --> 00:43:34,090
基于不确定性的评分是我在实践中看到最多的 很大程度上是因为它非常容易实现并且往往产生相当不错的结果

420
00:43:34,250 --> 00:43:36,050
但值得深入研究一下

421
00:43:36,210 --> 00:43:47,330
如果你决定沿着这条路走下去 如果你密切关注 你可能会注意到我们做数据管理的一些方式 我们挑选有趣数据点的方式 和我们做监控的方式之间有很多相似之处

422
00:43:47,490 --> 00:43:52,730
我认为这不是巧合 监控和数据管理是同一枚硬币的两面

423
00:43:52,890 --> 00:44:01,730
他们都感兴趣的是如何找到模型可能表现不好的数据点 或者我们不确定模型在这些数据点上表现如何的数据点

424
00:44:01,890 --> 00:44:07,170
例如 用户驱动策展是监控用户反馈指标的另一个方面

425
00:44:07,330 --> 00:44:09,050
这两件事都是基于相同的参数

426
00:44:09,210 --> 00:44:19,730
分层抽样很像做子组或队列分析，确保我们从重要的子组中获得足够的数据点，或者确保我们的指标在这些子组中没有下降。

427
00:44:19,890 --> 00:44:31,090
投影用于数据管理和监控，将高维数据分解成我们认为有趣的分布，

428
00:44:31,250 --> 00:44:39,650
然后是主动学习。一些技术在监测中也有镜像，比如预测非分层数据点的损失，或者在该数据点上使用模型的不确定性。

429
00:44:39,810 --> 00:44:43,410
接下来 让我们讨论一些在实践中如何进行数据管理的案例研究

430
00:44:43,570 --> 00:44:49,840
第一篇是关于openAI如何训练Dall-e-2来检测模型的恶意输入的博客文章。

431
00:44:50,000 --> 00:44:51,640
他们在这里使用了两种技巧

432
00:44:51,800 --> 00:44:57,200
他们使用主动学习 使用不确定性采样 为模型产生假阳性

433
00:44:57,360 --> 00:44:59,800
然后他们进行了人工管理

434
00:44:59,960 --> 00:45:09,970
实际上，他们是用一种自动化的方式来做的，但他们做了相似度搜索来找到与模型在特斯拉的下一个例子中表现不好的相似的例子。

435
00:45:10,100 --> 00:45:15,700
这是一个我很喜欢的演讲，来自Andre Carpathy，关于他们如何在特斯拉建立数据飞轮。他们用了两种技巧。

436
00:45:15,890 --> 00:45:20,290
一个是反馈循环 收集用户干预自动驾驶仪的信息

437
00:45:20,450 --> 00:45:25,050
然后第二个是人工管理，边缘案例检测的项目操作。

438
00:45:25,210 --> 00:45:35,010
这是非常酷的，因为他们实际上有基础设施，允许机器学习工程师，当他们发现一个新的边缘情况时，编写一个边缘情况检测器功能，然后实际部署在舰队上。

439
00:45:35,170 --> 00:45:41,170
这个边缘情况检测器不仅可以帮助他们管理数据，还可以帮助他们决定对哪些数据进行采样，这真的很强大。

440
00:45:41,330 --> 00:45:43,090
我想说的最后一个案例是克鲁兹的。

441
00:45:43,250 --> 00:45:48,540
他们也有建立一个持续机器的概念，他们这样做的主要方式是通过反馈循环。

442
00:45:48,700 --> 00:45:53,820
这是一些人用来建立这些数据管理系统的快速浏览

443
00:45:53,880 --> 00:45:57,940
在实践中 有一些工具正在出现 以帮助数据管理规模

444
00:45:58,100 --> 00:46:05,840
Nucleus和Aquarium是相对相似的专注于计算机视觉的工具，它们特别擅长基于最近邻的采样。

445
00:46:05,880 --> 00:46:10,520
Gantry还在开发一些工具 以帮助在各种不同的应用程序中实现这一点

446
00:46:10,780 --> 00:46:12,580
关于数据管理的具体建议。

447
00:46:13,000 --> 00:46:20,900
对于大多数用例来说，随机抽样可能是一个很好的起点，但如果你需要避免偏差，或者你的数据集中有罕见的类，

448
00:46:20,900 --> 00:46:27,700
你可能应该从分层抽样开始，或者至少在你开始抽样后很快引入分层抽样。

449
00:46:27,860 --> 00:46:37,380
如果你的机器学习系统中有一个反馈循环 我希望你能从中了解到有这些反馈循环是多么有用 那么用户驱动策展就是一个显而易见的选择

450
00:46:37,540 --> 00:46:43,660
这绝对是你应该做的事情 而且可能是在早期改进你的模型最有效的事情

451
00:46:43,820 --> 00:46:51,380
如果你没有一个反馈循环 那么使用基于自信的主动学习是下一个最好的选择 因为它很容易实现 在实践中也很有效

452
00:46:51,540 --> 00:46:57,260
最后 随着你的模型性能的提高 你将不得不越来越努力地寻找这些具有挑战性的训练点

453
00:46:57,420 --> 00:47:05,220
在一天结束的时候 如果您想从模型中获得最大的性能 就必须手动查看数据并试图找到有趣的故障模式

454
00:47:05,380 --> 00:47:06,780
没有什么能代替了解你的数据

455
00:47:06,940 --> 00:47:17,020
在我们将无限的未标记数据流整理成可以进行潜在训练的标记数据库之后，我们需要决定的下一件事是，我们要用什么触发器来进行再训练?

456
00:47:17,180 --> 00:47:22,550
这里的主要结论是，转向自动化再培训并不总是必要的。

457
00:47:22,710 --> 00:47:28,510
在许多情况下，只需手动重新训练就足够了，但它可以节省您的时间并导致更好的模型性能。

458
00:47:28,570 --> 00:47:31,670
所以 什么时候采取这种行动是有意义的 这是值得理解的

459
00:47:31,830 --> 00:47:38,590
迁移到自动化再训练的主要先决条件是能够在以相当自动化的方式进行再训练时再现模型性能

460
00:47:38,750 --> 00:47:46,730
如果你能够做到这一点 并且你不再非常积极地在这个模型上工作 那么可能值得实施一些自动再训练

461
00:47:46,890 --> 00:47:51,970
如果你发现自己非常频繁地重新训练这个模型 那么它可能会节省你更早实现它的时间

462
00:47:52,130 --> 00:47:59,490
当转向自动化训练时 主要的建议是保持简单 定期重新训练 比如每周一次 按照那个时间表重新训练

463
00:47:59,650 --> 00:48:02,170
然而 主要的问题是 你如何选择训练计划

464
00:48:02,300 --> 00:48:06,900
所以我建议做的是做一些测量来找出一个合理的再培训计划。

465
00:48:06,900 --> 00:48:15,500
你可以绘制出你的模型在一段时间内的表现，然后比较如果你在不同的频率上进行再训练，模型会有什么表现。

466
00:48:15,730 --> 00:48:20,530
你可以在这里做一些基本的假设 比如 如果你重新训练 你就能达到同样的准确度

467
00:48:20,600 --> 00:48:27,000
你在这里要做的是看看这些不同的再培训计划，看看这些曲线之间的面积，就像右上方的图表，

468
00:48:27,000 --> 00:48:34,900
这两条曲线之间的面积就是你的机会成本，也就是你没有更频繁地重新训练而导致的模型性能损失。

469
00:48:35,000 --> 00:48:42,600
一旦你有了不同再培训频率的不同机会成本，你就可以绘制出这些机会成本，

470
00:48:42,600 --> 00:48:54,200
然后你可以进行一些特别的练习来平衡，你知道，在我们从更频繁的再培训中获得的绩效收益之间的比率交易点在哪里

471
00:48:54,200 --> 00:49:04,700
再培训的成本，这既是运行再培训本身的成本，也是我们需要更频繁地评估模型而引入的运营成本，这是我建议在实践中做的。

472
00:49:05,000 --> 00:49:07,840
我对研究的一个要求是 我认为这很棒

473
00:49:08,000 --> 00:49:15,100
我认为有一种技术是非常可行的，它可以自动确定最佳的再训练策略，这种策略是基于表现如何衰退的，

474
00:49:15,100 --> 00:49:19,600
你对性能下降，运营成本和再培训成本有多敏感。

475
00:49:19,800 --> 00:49:25,520
所以我认为最终我们不需要每次都进行人工数据分析来确定再训练的频率

476
00:49:25,680 --> 00:49:30,420
如果你更高级，那么你可以考虑做的另一件事是基于性能触发器进行再训练。

477
00:49:30,430 --> 00:49:37,650
这看起来像是在准确性等指标上设置触发器 只有当准确性低于预定义阈值时才重新训练

478
00:49:37,810 --> 00:49:44,290
这样做的一大好处是 你可以更快地对正常训练计划之间发生的意外变化做出反应

479
00:49:44,450 --> 00:49:49,570
它是成本最优的 因为如果不能真正提高模型的性能 你可以跳过再训练

480
00:49:49,700 --> 00:49:58,500
但最大的缺点是，因为你事先不知道什么时候再培训，你需要有好的仪器和测量设备，以确保当你再培训时，

481
00:49:58,500 --> 00:50:02,300
你这样做的理由是正确的，而且新模型实际上做得很好。

482
00:50:02,490 --> 00:50:06,130
这些技术 我认为 也没有很多好的理论依据

483
00:50:06,290 --> 00:50:12,970
所以如果你是那种想要理解的人 你知道 为什么理论上这应该很好地工作 我认为你今天不会发现

484
00:50:13,130 --> 00:50:24,530
可能最重要的缺点是这增加了很多操作的复杂性，因为不是只知道，比如，嘿，早上8点，我知道我的再培训正在进行，所以我可以检查它，相反，这个再培训可以在任何时候发生。

485
00:50:24,690 --> 00:50:26,570
所以整个系统需要能够处理这个问题。

486
00:50:26,730 --> 00:50:30,290
这就引入了很多需要建立的新基础设施

487
00:50:30,450 --> 00:50:40,110
最后 有一个想法可能与你们大多数人无关 但值得思考 因为我认为它在未来会非常强大 那就是在线学习 你可以在每一个数据点上进行训练

488
00:50:40,130 --> 00:50:47,870
它在实践中并不常用 但在实践中使用得相当频繁的一种思想的放松是在线适应

489
00:50:48,030 --> 00:50:57,580
在线适应的工作方式不是对整个模型本身进行再训练 而是对模型之上的政策进行调整

490
00:50:57,600 --> 00:51:09,170
什么是政策 策略是一组规则 它采用模型所做的原始预测 比如分数 或者模型的原始输出 然后将其转化为用户看到的实际内容

491
00:51:09,330 --> 00:51:12,330
分类阈值是策略的一个例子

492
00:51:12,400 --> 00:51:18,200
或者，如果你的模型有很多不同的版本，你在进行组合，这些组合中有哪些，

493
00:51:18,200 --> 00:51:26,600
或者甚至这个特殊的请求将被路由到哪个版本的模型中进行在线适应，而不是在每个新数据点上重新训练模型

494
00:51:26,890 --> 00:51:33,730
相反，我们使用像多臂强盗这样的算法，随着更多数据的输入，在线调整该策略的权重。

495
00:51:33,890 --> 00:51:43,530
因此 如果您的数据在实践中变化非常频繁 或者您很难频繁地训练模型以适应 那么在线适应绝对值得研究

496
00:51:43,690 --> 00:51:56,780
接下来 我们已经触发了一个启动训练任务的触发器 我们需要回答的下一个问题是 在我们的数据库中的所有标记数据中 我们应该在哪些特定的数据点上训练这个特定的训练任务

497
00:51:56,940 --> 00:52:00,400
大多数时候，在深度学习中，我们只是在所有可用的数据上进行训练。

498
00:52:00,500 --> 00:52:08,700
但如果你有太多的数据无法做到这一点，那么根据数据的近代性是否是一个重要的信号来确定该数据是否有用，

499
00:52:08,700 --> 00:52:19,600
您可以滑动窗口以确保查看的是最新的数据(因此，在许多情况下是最有用的数据)，或者使用抽样或在线批处理选择等技术。

500
00:52:19,770 --> 00:52:27,890
如果没有，一个更高级的技巧是持续微调，这在今天的实践中很难实现，我们也会讲到。

501
00:52:28,050 --> 00:52:35,530
第一个选择是在所有可用的数据上进行训练 这样你就有了一个数据集 你可以跟踪你最后一个模型的训练数据集

502
00:52:35,690 --> 00:52:40,930
然后随着时间的推移 在上一次训练和下一次训练之间 你会得到一堆新的数据 你会整理其中的一些数据

503
00:52:40,990 --> 00:52:45,690
然后你就把所有的数据 添加到数据集中 然后在组合数据集上训练新模型

504
00:52:45,850 --> 00:52:50,970
所以这里的关键是你需要保持这个数据的版本控制 这样你就知道在每次训练迭代中添加了哪些数据

505
00:52:51,130 --> 00:52:57,330
如果你想正确地评估模型 跟踪你用来管理新数据的规则 这也很重要

506
00:52:57,490 --> 00:53:07,710
如果你的抽样方式不是均匀分布 你应该跟踪你用来抽样的规则这样你就能确定数据的来源

507
00:53:07,890 --> 00:53:13,410
第二个选择是使用滑动窗口将抽样偏向于最近的数据

508
00:53:13,570 --> 00:53:24,720
它的工作方式是 在你训练模型的每个点 你向后看 收集到当前时刻的数据窗口 然后在下一次训练时 你把这个窗口向前滑动

509
00:53:24,880 --> 00:53:34,840
这两个数据集之间可能会有很多重叠 但你有了所有的新数据 或者说很多新数据 然后把所有这些数据去掉 形成新的数据集

510
00:53:35,000 --> 00:53:41,600
这里要做的几件关键的事情是，查看新旧数据集之间的不同统计数据以发现错误非常有帮助，

511
00:53:41,600 --> 00:53:48,400
例如，如果您对其中一个列的特定分布进行了很大的更改，那么这可能表明引入了一个新的错误。

512
00:53:48,600 --> 00:53:57,520
你在这里会发现的一个挑战是比较旧版本和新版本的模型 因为它们不是在以非常直接的方式相关的数据上训练的

513
00:53:57,680 --> 00:54:06,400
如果你在一个需要采样数据的环境中工作 你不能对所有的数据进行训练 但没有任何理由相信最近的数据比旧的数据好得多

514
00:54:06,560 --> 00:54:13,720
然后 您可以使用各种技术从储层中采样数据 其中最有前途的是在线批量选择

515
00:54:13,880 --> 00:54:22,680
通常情况下，如果我们做的是随机梯度下降，我们会在每次训练迭代中进行小批量采样，直到我们用完数据，直到我们用完计算预算。

516
00:54:22,800 --> 00:54:30,700
在线批量选择 相反 我们所做的是 在每个训练步骤之前 我们抽取更大的一批样本 比我们最终想要训练的小批样本要大得多

517
00:54:30,920 --> 00:54:39,280
我们根据标签感知选择函数对小批量中的每个项目进行排序，然后根据该函数取前n个项目并对其进行训练。

518
00:54:39,440 --> 00:54:49,720
右边的论文描述了一个标签感知选择函数，称为可简化的保留损失选择函数，它在一些相对较大的数据集上表现得很好。

519
00:54:49,880 --> 00:54:52,720
所以如果你要研究这个技术 这可能是我开始的地方

520
00:54:52,880 --> 00:54:57,920
我们要讨论的最后一个选项是持续的微调，这在今天并不推荐。

521
00:54:58,080 --> 00:55:06,280
它的工作方式不是每次都从头开始重新训练 而是只在新数据上训练现有模型

522
00:55:06,440 --> 00:55:10,040
你可能想要这样做的主要原因是因为它更具成本效益

523
00:55:10,200 --> 00:55:12,400
右边的论文分享了grubHub的一些发现

524
00:55:12,560 --> 00:55:17,240
他们发现 与滑动窗相比 这种技术的成本提高了45倍

525
00:55:17,400 --> 00:55:22,640
但这里最大的挑战是 除非你非常小心 否则模型很容易忘记它过去学过的东西

526
00:55:22,800 --> 00:55:34,390
所以结果是 你需要有一个非常成熟的评估 能够非常小心地确保你的模型在所有类型的数据上都表现良好 在它值得实现这样的东西之前

527
00:55:34,550 --> 00:55:35,590
所以现在我们触发了再训练

528
00:55:35,750 --> 00:55:38,150
我们已经选择了用于训练工作的数据点

529
00:55:38,310 --> 00:55:46,910
我们已经训练了我们的模型，你知道，如果我们愿意，可以运行我们的超参数扫描，我们有一个新的候选模型，我们认为它已经准备好投入生产，下一步是测试这个模型。

530
00:55:47,070 --> 00:55:54,630
这个阶段的目标是生成一份报告，我们的团队可以在该报告上签字，以回答这个新模型是否足够好，或者是否比旧模型更好的问题。

531
00:55:54,790 --> 00:55:57,310
这里的关键问题是报告应该写些什么

532
00:55:57,400 --> 00:56:06,600
再说一次，这是一个没有很多标准化的地方，但我们在这里的建议是，将你当前的模型与之前的模型在以下所有方面进行比较，

533
00:56:06,600 --> 00:56:11,600
你关心的所有指标，你标记的所有数据切片或子集都很重要，

534
00:56:11,600 --> 00:56:18,800
你所定义的所有边缘情况，并以一种调整的方式来解释你的策展策略可能引入的任何样本和偏见。

535
00:56:18,990 --> 00:56:25,030
这种报告的一个例子如下所示，我们有所有的指标，在这种情况下，是准确性，精度和召回率。

536
00:56:25,190 --> 00:56:28,910
左边的是我们正在看的所有数据集和切片

537
00:56:28,900 --> 00:56:38,100
这里需要注意的是，我们有主要的验证集，就像大多数人用来评估模型的一样，但不仅仅是看这些数字的总和，

538
00:56:38,100 --> 00:56:42,200
我们也把它分成几个不同的类别，在这种情况下，

539
00:56:42,400 --> 00:56:45,600
用户的年龄和属于该用户的帐户的年龄。

540
00:56:45,760 --> 00:56:57,000
然后在主验证集下面，我们还有更具体的验证集，对应于特定的错误情况，我们知道这些错误情况给我们的模型带来了麻烦或者过去模型的一个版本的麻烦。

541
00:56:57,160 --> 00:57:07,520
这些可能只是你过去发现的特殊的边缘情况，比如你的模型处理语法错误的例子非常糟糕，或者它不知道一些z世代俚语的意思。

542
00:57:07,680 --> 00:57:14,640
这些是你在过去为你的模型发现的失败模式的例子，它们被纳入数据集，以测试你的模型的下一个版本和持续学习。

543
00:57:14,800 --> 00:57:19,020
就像训练集是动态的 随着时间的推移而变化

544
00:57:19,400 --> 00:57:21,080
评估集也是动态的

545
00:57:21,240 --> 00:57:26,800
当你整理新数据时 你应该将其中一些添加到你的训练集 但也应该将一些添加到你的评估集

546
00:57:26,960 --> 00:57:35,160
例如，如果你改变了抽样的方式，你可能想要添加一些新的样本数据到评估集，以确保你的评估集代表新的抽样策略。

547
00:57:35,320 --> 00:57:45,200
或者，如果你发现了一个新的边缘情况，而不是仅仅将这个边缘情况添加到训练集中，它值得拿出一些边缘情况的例子作为一个特定的单元测试，作为离线评估套件的一部分。

548
00:57:45,360 --> 00:57:48,720
关于计算集是动态的这一事实 需要注意两个推论

549
00:57:48,880 --> 00:57:53,820
第一 你应该对你的评估集进行版本控制 就像你对训练集进行版本控制一样

550
00:57:54,000 --> 00:58:05,120
第二点是 如果你的数据发展得非常快 那么你保留的部分数据应该是最近的数据 过去一天或过去一个小时的数据 或者其他任何数据 以确保你的模型能够很好地推广到新数据

551
00:58:05,280 --> 00:58:06,660
一旦你有了基本的东西

552
00:58:06,840 --> 00:58:11,640
这里还有一个更高级的东西我认为很有前途的是期望测试

553
00:58:11,800 --> 00:58:16,760
期望检验的工作方式是选取一对已知关系的例子

554
00:58:16,920 --> 00:58:22,360
假设你正在做情感分析 你有一个句子说 我哥哥很好

555
00:58:22,520 --> 00:58:32,880
如果你让句子中的积极词更积极 而不是说 我的兄弟是伟大的 那么你会期望你的情感分类器对这个句子变得更加积极

556
00:58:33,040 --> 00:58:42,360
这些类型的测试已经在nlp和推荐系统中进行了探索，它们非常适合测试你的模型是否以可预测的方式进行泛化。

557
00:58:42,520 --> 00:58:50,210
因此 它们为您提供了更细粒度的信息 而不仅仅是关于您的模型如何处理以前未见过的数据的综合性能指标

558
00:58:50,300 --> 00:58:58,400
这里要做的一个观察是，就像数据管理与监控的高度相似一样，离线测试也是如此，就像在监控中一样，

559
00:58:58,400 --> 00:59:06,000
我们想要观察我们的指标，不仅是在总体上，而且在所有重要的数据子集和所有的边缘情况下。

560
00:59:06,170 --> 00:59:13,770
这两者之间的一个区别是 通常 您将在离线测试和在线测试中使用不同的度量

561
00:59:13,930 --> 00:59:17,410
你更有可能拥有离线可用的标签

562
00:59:17,570 --> 00:59:23,110
事实上 你总是有离线可用的标签 因为这就是你训练模型的方式

563
00:59:23,270 --> 00:59:25,270
但在网上 你更有可能得到反馈

564
00:59:25,400 --> 00:59:31,900
因此，尽管这两个概念非常相似，并且应该共享许多度量标准和子集的定义，

565
00:59:31,900 --> 00:59:38,100
在线监控和离线测试之间的一个摩擦点是度量标准有点不同。

566
00:59:38,310 --> 00:59:46,630
因此，我认为一个令人兴奋的研究方向是使用准确性等离线指标来预测用户参与度等在线指标。

567
00:59:46,790 --> 00:59:52,030
最后 一旦我们离线测试了候选模型 就该部署它并在线评估它了

568
00:59:52,190 --> 00:59:59,750
我们上次讨论过这个 我不想重复太多 但是作为一个提醒 如果你有基础设施能力这样做 那么

569
01:00:00,160 --> 01:00:08,910
你应该先在阴影模式下运行你的模型，然后再将其推向真实用户，然后运行A-B测试来确保这一点

570
01:00:09,280 --> 01:00:11,040
用户对它的反应比他们对旧模式的反应要好。

571
01:00:11,200 --> 01:00:14,830
然后 一旦你有了一个成功的a-b测试 将它推广给所有的用户

572
01:00:15,000 --> 01:00:19,200
但要循序渐进，最后，如果你在推出过程中发现问题，

573
01:00:19,320 --> 01:00:21,830
只是为了把它回滚到旧的模型，并试图找出哪里出了问题。

574
01:00:21,990 --> 01:00:31,890
因此，我们讨论了持续学习的不同阶段，从记录数据到管理数据，再到触发再培训，测试模型并将其投入生产。

575
01:00:32,050 --> 01:00:40,090
我们还谈到了监控和可观察性 这是关于给你一套规则 你可以用它来判断你的再训练策略是否需要改变

576
01:00:40,250 --> 01:00:54,590
我们观察到 在很多不同的地方 你在监测中学习的基本元素 比如预测 用户反馈和模型不确定性 对持续学习过程的不同部分也很有用

577
01:00:54,750 --> 01:00:59,310
这并非巧合 我认为监控和持续学习是同一枚硬币的两面

578
01:00:59,470 --> 01:01:04,590
我们应该利用监测到的信号直接改变我们的再训练策略

579
01:01:04,750 --> 01:01:13,170
所以我想做的最后一件事是通过一个你可能有的工作流的例子来让这个更具体一些 从发现模型中的问题到改变策略

580
01:01:13,330 --> 01:01:15,330
本节描述更多的特性状态。

581
01:01:15,490 --> 01:01:20,650
除非你在基础设施上投入了大量资金 否则在实践中很难让它感觉像这样无缝

582
01:01:20,810 --> 01:01:27,550
但我还是想提一下，因为我认为它为我们在持续学习的工作流程中所追求的东西提供了一个很好的最终状态。

583
01:01:27,770 --> 01:01:30,490
在你能够真正执行之前，你需要准备好的东西。

584
01:01:30,600 --> 01:01:39,900
我接下来要描述的是一个地方，用来存储和版本你的策略的所有元素，包括在线和离线测试的度量定义，

585
01:01:39,900 --> 01:01:47,800
这些指标的性能阈值，任何你想用于监控和数据管理的预测的定义，

586
01:01:47,800 --> 01:01:58,100
你认为特别重要的子群体或群组，可以根据定义你如何进行数据管理的逻辑(无论是抽样规则还是其他规则)打破你的参数。

587
01:01:58,450 --> 01:02:03,690
最后 你在每次不同的训练或评估中使用的特定数据集

588
01:02:03,850 --> 01:02:10,940
在我们的例子中，持续改进循环从警报开始，在这种情况下，警报可能是今天变得更糟的用户反馈。

589
01:02:11,100 --> 01:02:13,100
所以我们现在要做的就是弄清楚发生了什么

590
01:02:13,260 --> 01:02:17,940
接下来我们要用一些可观测性工具来研究这里发生了什么

591
01:02:18,100 --> 01:02:26,980
我们可能 你知道 进行一些分组分析 看看一些原始数据 找出问题实际上主要是孤立的新用户

592
01:02:27,140 --> 01:02:36,270
接下来我们可能要做的是进行错误分析，看看这些新用户和他们发送给我们的数据点，并尝试推断为什么这些数据点表现较差。

593
01:02:36,400 --> 01:02:41,500
我们可能会发现，我们的模型是假设人们会写电子邮件，

594
01:02:41,500 --> 01:02:48,100
但现在用户提交了一堆文本，这些文本中有一些通常在电子邮件中找不到的东西，比如表情符号，这导致了我们的模型问题。

595
01:02:48,270 --> 01:02:50,830
所以这是我们可能对再培训策略做出的第一个改变。

596
01:02:50,990 --> 01:02:58,430
我们可以将新用户定义为一群感兴趣的人 因为我们不希望新用户的性能再次下降而没有收到警告

597
01:02:58,500 --> 01:03:04,900
然后我们可以定义一个新的投影，帮助我们检测有表情符号的数据，并将该投影添加到我们的可观察性指标中，

598
01:03:04,900 --> 01:03:16,100
所以在未来的任何时候，如果我们想，作为调查的一部分，看看我们的表现在提交表情符号的用户和没有提交表情符号的用户之间是如何不同的，我们总是可以做到这一点，而不需要重写投影。

599
01:03:16,350 --> 01:03:25,630
接下来，我们可能会在存储库中搜索包含表情符号的历史示例，以便我们可以使用它们使我们的模型更好，然后通过添加该数据子集作为新的测试用例来调整我们的策略。

600
01:03:25,790 --> 01:03:30,790
所以现在 无论何时我们测试这个模型 我们都会看到它在表情符号数据上的表现

601
01:03:30,950 --> 01:03:38,270
除了添加表情符号示例作为测试用例 我们还会对它们进行整理 并将它们添加回我们的训练集中 并进行再训练

602
01:03:38,430 --> 01:03:47,830
一旦我们有了训练好的新模型，我们就会得到这个新的模型比较报告，它也将包括我们作为这个过程的一部分定义的新队列，以及我们定义的新的表情符号边缘情况数据集。

603
01:03:47,990 --> 01:03:53,150
最后 如果我们在做手动部署 我们可以部署那个模型 这就完成了持续改进循环

604
01:03:53,310 --> 01:03:55,470
总结一下，我想让你们从中得到什么

605
01:03:55,630 --> 01:04:05,600
持续学习是一个复杂、快速发展且难以理解的话题，所以如果你有兴趣了解生产机器学习的前沿是如何发展的，这是一个值得关注的领域。

606
01:04:05,700 --> 01:04:13,600
这节课的主要收获是我们分解了再培训策略的概念，它由许多不同的部分组成，指标的定义，

607
01:04:13,600 --> 01:04:20,300
兴趣投影的子集，帮助你分解和分析度量逻辑的高维数据性能阈值，

608
01:04:20,300 --> 01:04:26,200
用于管理新的数据集，以及你将用于高层次再培训和评估的特定数据集。

609
01:04:26,300 --> 01:04:35,880
一旦我们部署了模型的第一个版本 我们可以考虑作为机器学习工程师的角色的方式是使用我们定义的规则 作为我们的可观察性和监控套件的一部分

610
01:04:36,040 --> 01:04:43,280
迭代策略 对于你们中的许多人来说 在短期内 这与使用这些数据来重新训练模型没有什么不同 无论你想要什么

611
01:04:43,300 --> 01:04:51,600
但我认为，把它看作是一种策略，你可以在更高的层次上进行调整，这是一种有效的理解方式，因为你正在走向越来越自动化的再培训。

612
01:04:51,760 --> 01:04:59,040
最后 就像我们在本课程中讨论的ml生命周期的其他方面一样 我们在这里的主要建议是从简单开始 然后再添加复杂性

613
01:04:59,140 --> 01:05:04,120
在持续学习的背景下 这意味着手动重新训练你的模型是可以的

614
01:05:04,280 --> 01:05:14,760
随着您越来越先进 您可能希望自动化再训练 并且您还可能希望更智能地考虑如何采样数据 以确保您获得的数据对改进模型最有用

615
01:05:14,920 --> 01:05:16,860
这就是本周的全部内容 下次节目再见
