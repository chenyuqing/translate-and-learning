0
00:00:00,060 --> 00:00:04,160
大家好 欢迎回来 本周 我们将讨论将模型部署到生产环境中

1
00:00:04,320 --> 00:00:06,560
我们讨论的是生命周期的这一部分

2
00:00:06,720 --> 00:00:08,320
我们为什么要花整整一周的时间在这上面

3
00:00:08,480 --> 00:00:14,120
也许答案是显而易见的，对吧，那就是，如果你想建立一个机器学习能力的产品，你需要一些方法把你的模型投入生产。

4
00:00:14,280 --> 00:00:19,600
但我认为还有更微妙的原因，我认为部署模型是使模型良好的关键部分。

5
00:00:19,700 --> 00:00:24,870
首先，这样做的原因是，当您仅离线评估模型时，很容易忽略模型所具有的一些更细微的缺陷。

6
00:00:25,030 --> 00:00:30,590
它不能真正解决用户需要解决的问题。

7
00:00:30,750 --> 00:00:37,460
通常 当我们第一次部署一个模型时 只有那时我们才能真正看到这个模型是否真的做得很好

8
00:00:37,620 --> 00:00:44,890
但不幸的是，对于很多数据科学家和机器学习工程师来说，相对于你学过的其他一些技术，模型部署是一种事后的想法。

9
00:00:45,050 --> 00:00:57,130
这节课的目标是涵盖将模型部署到生产中的不同方式，我们不可能对它们都深入讲解，因为这是一个广泛而深刻的话题，可能值得一门课程本身。

10
00:00:57,290 --> 00:00:58,850
我个人也不是这方面的专家

11
00:00:59,010 --> 00:01:05,590
但我们要做的是，我们将涵盖一些愉快的路径，这些路径将带您在大多数用例中获得您的第一个模型。

12
00:01:05,750 --> 00:01:12,070
然后我们会带你参观一些其他的技巧如果你想做一些超出正常80%的事情，你可能需要学习。

13
00:01:12,090 --> 00:01:19,950
总而言之 将模型投入生产是非常重要的 因为只有在生产中 你才能看到它是否有效 是否解决了你要解决的任务

14
00:01:20,110 --> 00:01:32,540
我们要强调的是，你在这方面使用的技术和我们在生命周期的其他部分使用的技术很像，它的重点是，比如，尽早推出mvp，尽早部署，尽早部署最小可行模型，并经常部署。

15
00:01:32,700 --> 00:01:35,620
我们还将强调保持简单，并在稍后添加蓝色。

16
00:01:35,780 --> 00:01:40,200
所以我们将开始 我们将通过下面的过程 从建立一个原型开始

17
00:01:40,360 --> 00:01:46,680
然后我们会讲如何在UI中分离你的模型 这是你需要做的第一件事来制作更复杂的UI或缩放

18
00:01:46,840 --> 00:01:50,970
我们将讨论一些你需要做的技巧，以便扩展你的模型来服务许多用户。

19
00:01:51,130 --> 00:01:59,340
最后，我们将讨论更高级的技术，当你需要你的模型非常快时，你可能会使用，这通常意味着将它从web服务器移动到边缘。

20
00:01:59,500 --> 00:02:02,700
我要讲的第一件事是如何建立你的生产模型的第一个原型

21
00:02:02,720 --> 00:02:06,080
这里的目标只是一些你可以自己玩的东西和你的朋友分享

22
00:02:06,280 --> 00:02:11,120
幸运的是 不像我们第一次教这门课 有许多伟大的工具来构建模型的原型

23
00:02:11,280 --> 00:02:14,040
“拥抱脸”在他们的游乐场里内置了一些工具。

24
00:02:14,200 --> 00:02:23,960
他们最近还收购了一家名为gradio的公司，我们将在实验室中使用它，它可以很容易地在模型周围包装一个小用户界面。

25
00:02:24,120 --> 00:02:26,760
streamlit也是一个很好的工具。

26
00:02:26,920 --> 00:02:37,330
与gradio或hugging face空间相比，streamlit给了你更多的灵活性，但代价是你需要花更多的时间考虑如何在ui中整合所有的部分。

27
00:02:37,490 --> 00:02:40,080
但是使用一些最佳实践来思考仍然是很容易的

28
00:02:40,180 --> 00:02:45,800
当您首先部署原型模型时，我建议您为模型提供一个基本的UI，而不仅仅是一个api。

29
00:02:45,960 --> 00:02:54,840
这样做的原因是 你知道 这个阶段的目标是摆弄这个模型 从你自己和你的朋友 或者你的同事 或者任何与你交谈的人那里得到关于这个模型的反馈

30
00:02:54,940 --> 00:02:57,860
这个关于gradio和streamlit的项目在这里真的是你的朋友。

31
00:02:58,020 --> 00:03:04,280
Gradio通常就像添加几行代码来为模型创建一个简单的界面一样简单。

32
00:03:04,450 --> 00:03:18,010
Streamlit在这方面更有野心，它是一个工具，允许你只用Python构建非常复杂的ui，如果你是Python开发者，它会是你熟悉的界面，但你需要更多考虑如何构建东西，但仍然很简单。

33
00:03:18,170 --> 00:03:20,530
下一个最佳实践是不要只在笔记本电脑上运行

34
00:03:20,690 --> 00:03:22,650
在这个阶段，把它放在网址后面是值得的。

35
00:03:22,710 --> 00:03:24,190
为什么这很重要?

36
00:03:24,350 --> 00:03:25,670
一是更容易分享 对吧

37
00:03:25,690 --> 00:03:32,750
所以这里的部分目标是从其他人那里收集反馈，但是它也开始让你思考一些折衷，当你做一个更复杂的部署时你将会做的折衷。

38
00:03:32,910 --> 00:03:35,050
这个模型实际有多少延迟?

39
00:03:35,210 --> 00:03:40,210
幸运的是，有云版本的streamlit和hugging face，这使得这很容易。

40
00:03:40,370 --> 00:03:45,450
所以在这一点上，没有太多的理由不把它放在一个简单的url后面，这样你就可以和别人分享了。

41
00:03:45,610 --> 00:03:48,050
最后一个技巧就是在这个阶段不要压力太大。

42
00:03:48,210 --> 00:03:56,810
这是一个原型 如果你是第一次做这个 你可能不会花超过一天的时间 但如果你要做很多这样的模型 可能只需要几个小时

43
00:03:56,970 --> 00:03:59,130
我们已经讨论过第一步 即建立原型

44
00:03:59,290 --> 00:04:01,850
接下来我想谈谈为什么这行不通

45
00:04:02,010 --> 00:04:04,970
比如 为什么这不是你用来部署模型的最终解决方案

46
00:04:04,990 --> 00:04:12,570
那么，这将在哪里失败呢?第一件大事是，使用我们讨论的任何这些工具，您在如何为模型构建用户界面方面都将具有有限的灵活性。

47
00:04:12,730 --> 00:04:16,810
stremlit比gradio更灵活，但仍然相对有限。

48
00:04:16,970 --> 00:04:20,810
最终你会想要为模型建立一个完全自定义的UI。

49
00:04:20,970 --> 00:04:25,250
其次，这些系统往往不能很好地扩展到许多并发请求。

50
00:04:25,410 --> 00:04:28,690
所以如果只有你 或者你和几个朋友玩这个模型 那可能没问题

51
00:04:28,850 --> 00:04:32,490
但一旦你开始拥有用户 你很快就会达到这些的扩展限制

52
00:04:32,650 --> 00:04:40,970
这是一个很好的过渡，可以从高层次上讨论构建机器学习应用程序的不同方式，特别是模型适合该应用程序的地方。

53
00:04:41,130 --> 00:04:45,150
因此 我们将从一个应用程序外观的抽象图开始

54
00:04:45,310 --> 00:04:53,270
左边有一些不同的组件，我们有一个客户端，客户端本质上是你的用户，那是他们用来和你建立的应用交互的设备。

55
00:04:53,430 --> 00:05:00,470
它可以是浏览器 也可以是交通工具 无论他们与什么设备交互 那个设备都会通过网络向服务器发出呼叫

56
00:05:00,630 --> 00:05:08,390
那个服务器通常是如果你在构建一个web应用你的大部分代码都在运行，那个服务器会和一个数据库通信那里有数据存储用来驱动应用。

57
00:05:08,550 --> 00:05:13,190
有不同的方法来构建这个应用程序来适应机器学习模型

58
00:05:13,350 --> 00:05:23,930
我们刚刚描述的原型方法主要适用于这个服务方法中的模型 你托管的web服务器实际上只是在它里面有一个模型的打包版本

59
00:05:24,090 --> 00:05:31,970
当你编写流脚本或渐变脚本时，该脚本的一部分将用于加载模型，因此该脚本将构建你的UI并同时运行模型。

60
00:05:32,130 --> 00:05:34,650
所以这种模式 就像一个模式 有优点和缺点

61
00:05:34,810 --> 00:05:39,490
我认为，最大的好处是，如果你使用这些原型开发工具之一，它真的很简单。

62
00:05:39,650 --> 00:05:53,930
第二 即使你在做一些更复杂的事情 比如为你公司正在构建的应用重用你的web基础设施 你会重用很多现有的基础设施 所以它不需要你 作为一个模型开发者 设置很多新东西只是为了尝试你的模型

63
00:05:54,090 --> 00:05:57,510
这真的很棒。但这也有一些明显的缺点。

64
00:05:57,670 --> 00:06:09,950
第一个是你的web服务器，在很多情况下，比如一旦你超越了流光源和梯度类型，例如，它可能是用不同于你的模型的语言编写的，比如它可能是用Ruby或Javascript或类似的东西编写的。

65
00:06:10,110 --> 00:06:11,950
将您的模型转换为该语言可能很困难

66
00:06:12,110 --> 00:06:19,350
第二个原因是 通常情况下 特别是在构建模型的生命周期的早期 您的模型可能比您的服务器代码更改得更频繁

67
00:06:19,510 --> 00:06:29,870
因此 如果您有一个相对完善的应用程序 但是您仍在构建一个模型 那么您可能不希望在每次更新模型时都必须重新部署整个应用程序 这可能是每天 甚至一天多次

68
00:06:30,030 --> 00:06:34,190
这种方法的第三个缺点是它不能很好地按模型大小进行缩放

69
00:06:34,350 --> 00:06:54,470
如果你有一个非常大的模型 你试图运行推理 你必须加载您的web服务器上 所以要开始吃到web服务器的资源 并可能会影响用户体验的人们使用web服务器 即使他们不是相互作用模型 这不是主要的 他们在web应用程序中 因为所有的资源从web服务器被要求做这个模型运行

70
00:06:54,630 --> 00:07:03,510
第四个原因是服务器硬件 你可能运行web应用程序或移动应用程序的硬件 通常没有很好地优化机器学习工作负载

71
00:07:03,670 --> 00:07:07,390
特别地，你很少会在这些设备上有GPU。

72
00:07:07,550 --> 00:07:10,870
这可能是一个问题 也可能不是 这将在后面的课程中回到你们

73
00:07:11,030 --> 00:07:18,430
最后一个缺点是 你的模型本身和它的应用可能有非常不同的缩放属性 你可能希望能够以不同的方式缩放它们

74
00:07:18,590 --> 00:07:25,390
例如 如果你正在运行一个非常轻量级的系统 那么它可能不需要太多的资源或太多的思考来将其扩展到许多用户

75
00:07:25,550 --> 00:07:34,110
但如果你的模型本身非常复杂或非常大，你可能需要在这节课中使用一些高级技术并将这些模型托管在gpu上，以使它们规模化。

76
00:07:34,270 --> 00:07:37,710
你并不想把所有的复杂性都带到你的web服务器上

77
00:07:37,870 --> 00:07:44,330
当存在不同的缩放属性时 能够将这些关注点作为你正在构建的应用程序的一部分分离开来是很重要的

78
00:07:44,600 --> 00:07:48,520
这将我们带到了第二步 也就是将你的模型拉出UI

79
00:07:48,680 --> 00:07:52,160
有几种不同的方法可以做到这一点，我们在这里讨论两种不同的模式。

80
00:07:52,320 --> 00:07:57,160
第一种是将你的模型从UI中拉出来 让它直接与数据库交互

81
00:07:57,320 --> 00:07:59,380
这被称为批处理预测 这是如何工作的呢

82
00:07:59,720 --> 00:08:03,960
定期地 你会得到新的数据你会在每个数据点上运行你的模型

83
00:08:04,120 --> 00:08:07,600
然后将模型推断的结果保存到数据库中

84
00:08:07,760 --> 00:08:10,200
这在某些情况下非常有效 例如

85
00:08:10,360 --> 00:08:28,580
如果模型没有很多潜在的输入 如果你对每个用户或每个客户有一个预测 或者类似的情况 那么你可以以一定的频率重新运行你的模型 比如每小时 每天或每周 你可以有相当新鲜的预测返回给那些存储在数据库中的用户

86
00:08:28,740 --> 00:08:38,620
这类问题可以很好地发挥作用的例子是 在建立推荐系统的早期阶段 在某些情况下 做更多面向内部的用例 比如营销自动化

87
00:08:38,780 --> 00:08:49,940
例如，如果你想给每个营销线索打分，告诉你的营销和销售团队需要付出多少努力来关闭这些线索，那么你就会有这个有限的线索空间，需要对模型进行预测。

88
00:08:50,100 --> 00:08:56,300
所以你可以对数据库中每一个可能的主要故事运行一个模型预测 然后让你的用户从那里与它互动

89
00:08:56,460 --> 00:08:57,220
你要怎么做呢

90
00:08:57,240 --> 00:08:59,140
你是如何在时间表上运行模型的

91
00:08:59,300 --> 00:09:03,680
我们在上一讲中谈到的数据处理和工作流工具在这里也很好用

92
00:09:03,840 --> 00:09:07,560
你需要做的是重新运行你的数据预处理

93
00:09:07,720 --> 00:09:13,840
然后需要加载模型 运行预测 并将预测存储在应用程序使用的数据库中

94
00:09:14,000 --> 00:09:22,320
所以这就是一个有向的，循环的图形，一个数据操作的工作流，像Dagster气流或前缀这样的工具就是用来解决的。

95
00:09:22,480 --> 00:09:30,720
这里值得学习的是，还有像metflow这样的工具是为机器学习或数据科学用例设计的，这可能是一种更容易入门的方法。

96
00:09:30,880 --> 00:09:36,440
那么 这种离线运行模型并将预测结果放入数据库的模式的优缺点是什么呢

97
00:09:36,600 --> 00:09:38,960
最大的好处是 这真的很容易实现 对吧

98
00:09:38,980 --> 00:09:49,800
它重用了这些现有的批处理工具 你可能已经使用这些工具来训练你的模型 而且它不需要你托管任何类型的新web服务器来向用户提供这些预测

99
00:09:49,960 --> 00:09:53,120
你可以把预测放到你的产品已经在使用的数据库中

100
00:09:53,280 --> 00:09:59,670
它也很容易扩展 因为数据库本身的设计和几十年来的工程设计非常容易扩展

101
00:09:59,830 --> 00:10:05,710
这似乎是一个简单的模式，但它被大型公司的大规模生产系统所使用。

102
00:10:05,990 --> 00:10:07,890
多年来一直如此。

103
00:10:08,150 --> 00:10:10,350
这通常是针对推荐系统之类的东西

104
00:10:10,510 --> 00:10:15,230
这是一个久经考验的真实模式 你可以运行 并且非常有信心它会很好地工作

105
00:10:15,390 --> 00:10:17,270
它的延迟也相对较低

106
00:10:17,450 --> 00:10:22,750
因为数据库本身是为与最终应用程序交互而设计的

107
00:10:22,910 --> 00:10:26,730
所以延迟是数据库设计人员能够为我们解决的一个问题

108
00:10:26,900 --> 00:10:32,020
这种方法也有一些非常明显的缺点 最重要的是它并不适用于所有类型的模型

109
00:10:32,080 --> 00:10:40,940
如果你的模型有复杂的输入 如果输入的范围太大 每次你需要更新你的预测时都不能枚举 那么这就行不通了

110
00:10:41,100 --> 00:10:45,420
第二个缺点是你的用户不会从你的模型中得到最新的预测

111
00:10:45,440 --> 00:10:56,100
如果该特性要进入模型，比如每小时、每分钟或每秒钟更改一次，但是您每天只运行这个批处理预测作业，那么您的用户看到的预测可能有点过时。

112
00:10:56,260 --> 00:10:58,100
在推荐系统的背景下考虑这个问题。

113
00:10:58,260 --> 00:11:09,540
如果你只是每天运行推荐系统的预测，那么你提供给用户的那些推荐不会考虑到这些用户在预测之间提供给你的所有联系人。

114
00:11:09,700 --> 00:11:17,020
所以他们今天看的电影 他们今天看的电视节目 这些都不会被考虑在内 至少他们推荐的机器学习部分不会被考虑在内

115
00:11:17,180 --> 00:11:21,900
但是 你知道 还有其他的算法方法来确保你不会做一些事情 比如给用户看两次同样的电影

116
00:11:22,060 --> 00:11:25,300
最后一个缺点是模型经常会过时

117
00:11:25,460 --> 00:11:42,740
如果你的批处理作业 由于某种原因 某个数据预处理步骤超时了新的预测没有被转储到数据库中 这类事情会让没有得到最新预测的问题变得越来越严重 而且很难检测到 尽管有数据质量工具可以帮助检测到

118
00:11:42,900 --> 00:11:51,100
他们要讨论的下一个模式是 与其离线运行模型 把预测结果放在数据库中 不如在线运行模型 作为它自己的服务

119
00:11:51,260 --> 00:11:59,620
服务将与后端或客户端进行交互通过向这个模型服务发出请求，发送，嘿，这个特定输入的预测是什么?

120
00:11:59,780 --> 00:12:04,700
并收到回应 模型说这个输入的预测值是这个特定的值

121
00:12:04,860 --> 00:12:07,380
这种方法的优点是它是可靠的

122
00:12:07,540 --> 00:12:12,940
如果你的模型有bug 如果你的模型直接在web服务器上运行 那么你的整个应用程序就会崩溃

123
00:12:13,100 --> 00:12:17,580
但在应用程序中将其作为独立服务托管意味着这种可能性较小

124
00:12:17,740 --> 00:12:28,580
第二，它更具可扩展性，所以你可以选择最好的硬件，为模型本身设置的最好的基础设施，并根据需要进行扩展，而无需担心这会如何影响应用程序的其余部分。

125
00:12:28,740 --> 00:12:36,860
第三，它非常灵活。如果您为特定模型建立了模型服务，那么您可以在其他应用程序或应用程序的其他部分中非常容易地重用该服务。

126
00:12:37,020 --> 00:12:40,100
由于这是一个单独的服务 您可以添加一个网络呼叫

127
00:12:40,260 --> 00:12:48,420
当您的服务器或客户端与模型交互时，它必须通过网络发出请求并接收响应，因此这会给您的应用程序增加一些延迟。

128
00:12:48,580 --> 00:12:57,060
相对于我们之前讨论过的其他技术 它增加了基础设施的复杂性 因为现在您需要托管和管理一个单独的服务来托管您的模型

129
00:12:57,220 --> 00:13:01,100
我认为，对于很多机器学习团队来说，真正的挑战是，嘿，我擅长训练模型。

130
00:13:01,120 --> 00:13:03,380
我不知道如何运行网络服务

131
00:13:03,540 --> 00:13:09,660
然而，我确实认为这是大多数ML驱动产品的最佳选择，因为其他方法的缺点实在太大了。

132
00:13:09,820 --> 00:13:16,740
在大多数复杂的用例中 您确实需要能够独立于应用程序本身来缩放模型

133
00:13:16,900 --> 00:13:23,500
对于机器学习的许多有趣的用途 不要有一个有限的输入宇宙 我们可以每天枚举

134
00:13:23,660 --> 00:13:30,180
我们真的需要能够让我们的用户向我们发送他们想要得到的任何请求 并收到定制的响应

135
00:13:30,340 --> 00:13:35,300
在下一节中 我们将讨论如何构建模型服务的基础知识 其中包括一些组件

136
00:13:35,460 --> 00:13:40,700
我们将讨论restapi 这是您的服务将用于与应用程序的其余部分交互的语言

137
00:13:40,860 --> 00:13:46,060
我们将讨论依赖管理，因此如何处理这些可能需要升级的pytorch或tensorflow的讨厌版本。

138
00:13:46,220 --> 00:13:49,900
我们将讨论性能优化 如何使它运行得快 可扩展性好

139
00:13:50,060 --> 00:13:55,380
然后我们将讨论推出 即一旦准备好部署模型 如何将模型的下一个版本投入生产

140
00:13:55,540 --> 00:14:03,940
最后 一旦我们涵盖了你们需要考虑的技术考虑 我们会讨论管理选项它会为你们解决很多技术问题

141
00:14:04,100 --> 00:14:05,460
首先 让我们谈谈restapi

142
00:14:05,620 --> 00:14:08,240
什么是rest api ?Rest api服务

143
00:14:08,660 --> 00:14:18,730
预测对标准格式的http请求的响应，还有其他替代协议可用于与基础设施上托管的服务进行交互。

144
00:14:18,890 --> 00:14:25,010
可能你在ml中看到的最常见的是gRPC，它被用于许多谷歌产品，如Tensorflow服务图。

145
00:14:25,170 --> 00:14:30,670
QL是web开发中另一种非常常用的协议 它与构建模型服务并不十分相关

146
00:14:30,830 --> 00:14:32,670
那么RestApi是什么样的呢

147
00:14:32,830 --> 00:14:42,500
你可能以前见过这样的例子，但是当你发送数据到一个格式化为json blob的web url时，通常这是一个rest请求。

148
00:14:42,660 --> 00:14:45,740
这是与rest api交互的一个示例。

149
00:14:45,900 --> 00:14:51,300
在这个示例中 我们将一些数据发送到这个url 它是托管rest api的地方

150
00:14:51,460 --> 00:14:53,180
api.fullstackdeeplearning.com

151
00:14:53,340 --> 00:15:01,780
我们使用post功能 它是Rest标准的一部分它告诉服务器它将如何与我们发送的数据交互

152
00:15:01,940 --> 00:15:08,530
然后我们发送这个json数据块它代表我们想要从中接收预测的模型的输入。

153
00:15:08,690 --> 00:15:13,720
所以你可能会问的一个问题是我们发送给模型的输入格式有什么标准吗

154
00:15:13,880 --> 00:15:16,080
不幸的是 目前还没有真正的标准

155
00:15:16,240 --> 00:15:18,670
下面是一些来自rest api的示例。

156
00:15:18,920 --> 00:15:23,080
对于托管在主要云中的模型服务。

157
00:15:23,240 --> 00:15:27,800
我们将看到它们期望模型输入格式的不同之处

158
00:15:27,960 --> 00:15:37,560
例如在Google Cloud中。他们期望得到一批输入，这些输入被结构化为他们称为实例的列表，每个实例都有值和一个键。

159
00:15:37,720 --> 00:15:43,880
在azure中，他们期望的是一列被称为数据的东西，其中数据结构本身取决于你的模型架构是什么。

160
00:15:44,040 --> 00:15:50,630
在sagemaker中，它们也需要实例，但是这些实例的格式与Google Cloud中的不同。

161
00:15:50,700 --> 00:15:59,400
因此，我希望在未来看到的一件事是朝着为机器学习服务调用rest API的标准接口发展。

162
00:15:59,590 --> 00:16:05,260
由于您可能发送到这些服务的数据类型非常有限 因此我们应该能够开发一个行业标准

163
00:16:05,420 --> 00:16:08,060
下一个主题将讨论依赖管理模型

164
00:16:08,220 --> 00:16:28,290
预测不仅依赖于你运行预测的模型的权重 还依赖于用来将这些权重转化为预测的代码 包括预处理和依赖项 运行你调用的函数所需的特定库版本为了让你的模型做出正确的预测 所有这些依赖项都需要出现在你的web服务器上

165
00:16:28,470 --> 00:16:36,510
不幸的是 依赖关系通常是web应用程序(特别是机器学习web服务)中臭名昭著的麻烦原因

166
00:16:36,670 --> 00:16:38,190
原因有几点

167
00:16:38,350 --> 00:16:42,710
首先 它们很难在您的开发环境和服务器之间保持一致

168
00:16:42,870 --> 00:16:59,570
你如何确保服务器运行的是Tensorflow, pytorch, scikit-Learn, numpy，以及你所依赖的其他库的完全相同的版本，当你训练这些模型时，你的木星笔记本是很难更新的，如果你在一个环境中更新依赖项，你需要在所有环境中更新它们。

169
00:16:59,730 --> 00:17:08,410
特别是在机器学习中 因为很多库都发展得非常快 像一个紧张的 低版本的小变化可以改变你的模型的行为

170
00:17:08,430 --> 00:17:12,290
所以在高层次上 特别注意ML中的这些版本是很重要的

171
00:17:12,350 --> 00:17:15,310
有两种策略可以处理依赖性

172
00:17:15,530 --> 00:17:22,810
第一种是约束你的模型的依赖关系 以一种不可知的格式保存你的模型 这种格式可以在任何地方运行

173
00:17:22,970 --> 00:17:29,130
第二步是将整个推理，程序，模型的整个预测函数，打包到一个容器中。

174
00:17:29,290 --> 00:17:32,530
我们来讨论一下如何约束模型的依赖关系

175
00:17:32,690 --> 00:17:37,610
现在人们做这件事的主要方式是通过一个叫做ONNX的直播，即开放神经网络交换。

176
00:17:37,770 --> 00:17:42,090
ONNX的目标是成为机器学习模型的互操作性标准。

177
00:17:42,250 --> 00:17:51,290
他们希望你能够用任何语言定义神经网络 并在任何地方一致地运行它 无论你使用什么推理框架 使用什么硬件等等

178
00:17:51,450 --> 00:18:04,290
这就是承诺 现实情况是 由于底层库用于构建 这些模型当前变化如此之快 因此在这个转换层中经常存在错误 并且在许多情况下 这会产生比它实际为您解决的问题更多的问题

179
00:18:04,450 --> 00:18:09,890
另一个开放性的问题是 在很多情况下 这并不能处理非库代码

180
00:18:09,910 --> 00:18:20,490
在机器学习中，像特征变换，图像变换这样的东西，你可以作为你的tensorflow或pytorch图的一部分来做，但你也可以作为一个Python函数来包装这些东西。

181
00:18:20,650 --> 00:18:26,550
而这些开放的神经网络标准，比如onnx，并没有一个关于如何处理预处理的好故事。

182
00:18:26,730 --> 00:18:30,290
这就引出了管理依赖关系的第二种策略 即容器

183
00:18:30,450 --> 00:18:32,850
如何管理像Docker这样的容器的依赖关系

184
00:18:33,010 --> 00:18:34,010
我们会讲到一些东西

185
00:18:34,170 --> 00:18:40,410
我们将讨论Docker和一般虚拟机之间的区别 您可能已经在计算机科学课程中讨论过了

186
00:18:40,570 --> 00:18:45,330
我们将讨论如何通过docker文件构建docker映像并通过层构建docker映像。

187
00:18:45,490 --> 00:18:50,810
我们将讨论一下生态系统，然后我们将讨论可以用于机器学习的docker的特定包装器。

188
00:18:50,970 --> 00:19:05,090
要了解docker的第一件事是它与虚拟机的不同之处，这是在虚拟机中打包依赖项的一种较老的技术，本质上是打包整个操作系统以及构建在该操作系统之上的所有库和应用程序。

189
00:19:05,250 --> 00:19:10,610
所以它往往是一个非常沉重的负担 因为操作系统本身就是大量的代码 运行起来很昂贵

190
00:19:10,770 --> 00:19:17,290
docker的改进之处在于不再需要将操作系统与应用程序一起打包。

191
00:19:17,450 --> 00:19:33,170
相反 你把库和应用程序打包在一个叫做容器的东西里 然后你有一个Docker引擎 它运行在你的操作系统上 基于你的笔记本电脑或服务器 它知道如何虚拟化操作系统 并在上面运行你的bin 库和应用程序

192
00:19:33,190 --> 00:19:36,650
所以我们知道Docker比典型的虚拟机轻量级得多。

193
00:19:36,810 --> 00:19:40,930
由于轻量级 它的使用方式与其他的虚拟机非常不同

194
00:19:41,090 --> 00:19:46,450
特别是，常见的模式是为每一个独立的任务启动一个新的docker容器。

195
00:19:46,610 --> 00:19:47,770
是的 这是你应用的一部分

196
00:19:47,930 --> 00:19:54,290
因此 例如 如果你正在构建一个web应用程序 你不会像使用虚拟机那样只有一个Docker容器

197
00:19:54,450 --> 00:20:01,090
相反，你可能有四个。您可能有一个用于web服务器自身，一个用于数据库，一个用于作业队列，一个用于您的worker。

198
00:20:01,110 --> 00:20:09,050
由于应用程序的每个部分都提供不同的功能 因此具有不同的库依赖关系 也许在将来 您可能需要对其进行不同的扩展

199
00:20:09,210 --> 00:20:18,300
每个容器都有自己的容器，这些容器作为编排系统的一部分一起运行，稍后我们会讲到如何创建一个docker容器?

200
00:20:18,460 --> 00:20:20,820
Docker容器是从Docker文件创建的。

201
00:20:20,980 --> 00:20:22,500
这就是docker文件的样子。

202
00:20:22,660 --> 00:20:26,380
它运行一系列步骤来定义要在其中运行代码的环境。

203
00:20:26,540 --> 00:20:32,830
因此，在这种情况下，它正在导入另一个容器，该容器具有一些用于运行Python2.7的预打包依赖项。

204
00:20:33,150 --> 00:20:35,910
希望你没有运行Python2.7，但如果你运行Python2.7，

205
00:20:36,090 --> 00:20:37,810
你可以构建一个docker容器来使用它，

206
00:20:38,130 --> 00:20:47,650
在顶部使用这个from命令 然后做其他事情 比如从本地机器上添加数据 安装包 公开端口和运行实际的应用程序

207
00:20:47,810 --> 00:20:53,610
您可以在笔记本电脑上构建这些Docker容器，并在进行开发时将它们存储在那里。

208
00:20:53,770 --> 00:21:08,670
但是，Docker真正强大的一点是，它还允许您从托管在其他服务器、Docker服务器或您的云提供商(例如，您运行Docker的方式)上的Docker中心构建、存储和提取Docker容器

209
00:21:08,930 --> 00:21:11,330
容器通常是通过使用这个Docker运行命令

210
00:21:11,490 --> 00:21:18,770
它会做的是，在这个例子中，它会找到右边这个容器叫做Gordon Slash，正在开始，第二部分，它会尝试运行这个容器。

211
00:21:18,930 --> 00:21:40,130
但是如果你连接到一个Docker Hub而你在本地没有Docker镜像，那么它会做的是它会自动从你连接到这个服务器的Docker Hub中拉出它，然而，你的Docker引擎连接到这个服务器，我会下载那个Docker容器，它会在你的本地机器上运行它，所以你可以实验那个代码环境它将与你部署在服务器上的代码环境相同。

212
00:21:40,290 --> 00:21:44,490
更详细地说，docker被分成三个不同的组件。

213
00:21:44,650 --> 00:21:56,370
第一个是客户端。这是您将在笔记本电脑上运行的，用于从本地定义的docker文件构建映像，提取您想要在笔记本电脑上运行某些代码的映像，在映像中运行命令。

214
00:21:56,530 --> 00:22:01,970
这些命令实际上是由一个docker主机执行的，它通常在您的笔记本电脑上运行，但不是必须这样。

215
00:22:02,130 --> 00:22:11,130
如果您想要更多的存储空间或更高的性能，它也可以在服务器上运行，然后docker主机与注册表进行通信，注册表是存储您可能想要访问的所有容器的地方。

216
00:22:11,290 --> 00:22:20,770
这种关注点分离是使docker真正强大的原因之一，因为您不受笔记本电脑上构建拉取和运行docker映像的计算和存储数量的限制。

217
00:22:20,930 --> 00:22:27,090
并且您不受您在docker主机上可以访问的内容的限制来决定运行哪些映像。

218
00:22:27,250 --> 00:22:33,250
事实上，在不同的公共docker中心上有一个非常强大的docker映像生态系统。

219
00:22:33,410 --> 00:22:36,450
您可以很容易地找到这些图像 修改它们并将它们贡献回来

220
00:22:36,610 --> 00:22:40,810
并且拥有互联网上所有正在构建docker文件和docker映像的人的全部权力。

221
00:22:40,970 --> 00:22:43,770
可能只有一个已经开箱即用地解决了您的用例。

222
00:22:43,930 --> 00:22:46,510
在同一个地方存储私有图像也很容易

223
00:22:46,670 --> 00:22:54,350
因此，由于docker的社区和轻量级特性，它在近年来变得非常流行，并且在这一点上几乎无处不在。

224
00:22:54,510 --> 00:22:59,660
因此 如果您正在考虑为部署打包依赖项 那么这可能就是您想要使用的工具

225
00:22:59,820 --> 00:23:02,260
Docker并不像听起来那么难上手。

226
00:23:02,420 --> 00:23:08,220
您需要阅读一些文档，并稍微摆弄一下docker文件，以了解它们是如何工作的以及它们是如何组合在一起的。

227
00:23:08,380 --> 00:23:15,060
由于docker hub的存在，您通常不需要构建自己的docker，并且您可以在开始时下载一个已经适用于您的用例的docker。

228
00:23:15,220 --> 00:23:17,780
话虽如此，docker还是有一些学习曲线的。

229
00:23:17,940 --> 00:23:20,700
如果我们研究的是机器学习 有没有什么方法可以简化这个

230
00:23:20,860 --> 00:23:25,500
有很多不同的开源包就是专门为这个目的设计的

231
00:23:25,660 --> 00:23:29,740
一个叫COG 另一个叫BentoML 第三个叫Truss

232
00:23:29,900 --> 00:23:41,820
这些都是由不同的模型托管提供商构建的，它们被设计成可以很好地与它们的模型托管服务一起工作，但也只是打包你的模型及其所有依赖项和标准docker容器格式，所以你可以在任何你想要的地方运行它。

233
00:23:41,980 --> 00:23:44,780
这些系统的工作方式有两个组成部分

234
00:23:44,940 --> 00:23:48,860
第一 有一个标准的方法来定义你的预测服务

235
00:23:49,020 --> 00:23:50,820
所以你就像 预测函数的模型

236
00:23:50,980 --> 00:23:53,500
您如何以一种服务可以理解的方式包装它

237
00:23:53,660 --> 00:23:56,860
在COG中，你可以在左下角看到基本预测器类。

238
00:23:57,020 --> 00:24:01,780
在truss中，它依赖于你正在使用的模型库，就像你在右侧看到的那样。

239
00:24:01,940 --> 00:24:04,100
这是第一件事，你如何包装这个model.predict功能。

240
00:24:04,260 --> 00:24:16,260
第二个是一个黄色的文件，它定义了其他依赖关系和包版本，这些依赖关系和包版本将进入这个docker容器，它将在你的笔记本电脑上或远程运行。

241
00:24:16,420 --> 00:24:25,740
所以，这是一个简化版本的步骤，你会把它放进你的docker构建命令中，但是在一天结束的时候，它以标准格式打包，你可以部署到任何地方。

242
00:24:25,900 --> 00:24:40,780
所以 如果你想拥有使用Docker的一些优势 使你的机器学习模型可复制并部署它们 但你不想真正经历学习Docker的学习曲线 或者你只是想要一些更自动化的机器学习用例 那么这三个库值得一试

243
00:24:40,940 --> 00:24:43,500
我们将讨论的下一个主题是性能优化

244
00:24:43,660 --> 00:24:45,500
那么我们如何让模型运行

245
00:24:45,660 --> 00:24:46,620
我们怎么让它们跑得快

246
00:24:46,780 --> 00:24:48,380
在这里我需要回答几个问题

247
00:24:48,540 --> 00:24:51,720
第一个问题是 我们是否应该使用GPU进行推理

248
00:24:51,910 --> 00:24:56,890
我们将讨论并发性、模型蒸馏、量化、缓存、批处理、共享gpu。

249
00:24:57,050 --> 00:24:59,970
最后 库为你自动化了很多这些东西

250
00:25:00,130 --> 00:25:10,290
所以这节课的精神将是一种旋风式的旅行 通过一些主要的技术使你的模型运行得更快 并试图给你一些提示 你可以去哪里学习更多关于这些主题的知识

251
00:25:10,450 --> 00:25:14,610
您可能会问的第一个问题是，应该将模型托管在GPU上还是CPU上?

252
00:25:14,770 --> 00:25:17,050
在Gpu上托管模型有一些优势

253
00:25:17,210 --> 00:25:23,890
第一，它可能和你训练模型的硬件是一样的，这样可以消除一些损失和平移类型的力矩。

254
00:25:24,050 --> 00:25:30,410
第二大弊端是 当你的模型变得非常大 当你的技术变得相对先进时 你的流量就会变得非常大

255
00:25:30,570 --> 00:25:33,010
这通常是获得最大吞吐量的方式。

256
00:25:33,170 --> 00:25:38,530
就像同时访问你的模型的大多数用户一样 是通过将模型托管在Gpu上

257
00:25:38,690 --> 00:25:40,930
但是GPS也引入了很多复杂性

258
00:25:41,090 --> 00:25:49,750
它们的设置更复杂，因为它们在托管web服务的道路上不像cpu那样成熟，而且它们通常(几乎总是)实际上更昂贵。

259
00:25:49,980 --> 00:25:51,550
所以我认为有一点值得强调。

260
00:25:51,940 --> 00:26:01,460
因为这是我一直看到的一个常见的误解，只是因为你的模型是在Gpu上训练的，并不意味着你需要在Gpu上实际托管它才能让它工作。

261
00:26:01,620 --> 00:26:08,460
所以要仔细考虑你是否真的需要一个GPU 或者你是否更好 特别是对于你的模型的早期版本 仅仅把它托管在CPU上

262
00:26:08,620 --> 00:26:15,580
事实上 通过使用其他一些技术 可以以相对较低的成本从CPU推理中获得非常高的吞吐量

263
00:26:15,740 --> 00:26:17,960
其中一个主要的是并发性。

264
00:26:18,180 --> 00:26:26,450
并发意味着在一台主机上，不只是有一个模型副本在运行，我们有多个模型副本在不同的CPU或不同的CPU核心上并行运行。

265
00:26:26,770 --> 00:26:28,930
你要怎么做呢?

266
00:26:29,090 --> 00:26:40,930
那一年你需要注意的主要技术是线程调优 所以要确保在Torch中 它知道你需要使用哪些线程来实际运行模型 否则不同的Torch模型将会在你的机器上竞争线程

267
00:26:41,090 --> 00:26:47,330
roblox有一篇很棒的博客文章，讲的是他们如何将BERT扩展到每天只使用cpu就能处理10亿个请求。

268
00:26:47,490 --> 00:26:50,890
他们发现这比使用gpu要简单得多，成本也低得多。

269
00:26:51,050 --> 00:26:55,210
cpu也可以非常有效地扩展到高吞吐量

270
00:26:55,370 --> 00:26:56,960
你不一定需要gpu来做这些。

271
00:26:57,120 --> 00:26:59,080
下一个要介绍的技术是模型蒸馏

272
00:26:59,240 --> 00:27:08,520
什么是模型蒸馏?模型蒸馏意味着，一旦你有了你训练的模型，可能是一个非常大或非常昂贵的模型，它在你想要解决的任务中表现得非常好。

273
00:27:08,680 --> 00:27:12,960
你可以训练一个较小的模型来模仿较大模型的行为

274
00:27:13,120 --> 00:27:23,040
所以这通常是一种把你的大模型学到的知识压缩到一个小得多的模型中的方法也许你无法从头开始训练到同样的性能

275
00:27:23,200 --> 00:27:25,650
但一旦你有了更大的模型 它就能模仿它

276
00:27:25,810 --> 00:27:30,250
这有用吗 我将向您指出这篇博客文章 其中涵盖了如何做到这一点的几种技巧

277
00:27:30,410 --> 00:27:37,170
值得注意的是 您自己可能很难做到这一点 而且在生产实践中很少这样做

278
00:27:37,330 --> 00:27:41,730
一个很大的例外是，经常有流行模型的蒸馏版本。

279
00:27:41,800 --> 00:27:50,600
DistilBERT就是一个很好的例子它是预先训练好的，你可以用它来进行有限的性能权衡。我们要讲的下一个技术是量化。

280
00:27:50,770 --> 00:28:13,040
这是什么?这意味着，当你用你的模型做预测的时候，你不用做所有的矩阵乘法运算，也不用全精度的64位或32位浮点数，你的模型权重可能存储在这些浮点数中，取而代之的是，你可以用较低保真度的数字来执行其中的一些运算，或者可能是所有的运算。

281
00:28:13,200 --> 00:28:17,480
所以这些可以是16位浮点数，甚至在某些情况下，是8位整数。

282
00:28:17,640 --> 00:28:25,880
这引入了一些准确性的权衡，但通常这是一个值得做的权衡，因为失去的准确性相对于你获得的性能来说是非常有限的。

283
00:28:26,040 --> 00:28:34,280
你怎么能这么做?推荐的方法是使用pytorch、hugs face和tensorflow lite中的内置方法，而不是尝试自己滚动这些方法。

284
00:28:34,440 --> 00:28:47,560
即使是在训练模型的时候也要开始考虑这个问题，因为量化技术，有意识的训练，可以提高量化模型的准确性，而不是单纯地训练模型，然后在事后再进行量化。

285
00:28:47,720 --> 00:28:54,200
我想特别指出一个工具来做这个 它是相对较新的最优库 它让这个变得很简单

286
00:28:54,360 --> 00:28:58,270
所以如果你已经在用拥抱脸模型了，尝试一下也没什么坏处。

287
00:28:58,430 --> 00:29:01,470
接下来，我们将讨论缓存。什么是机器学习模型的缓存?

288
00:29:01,630 --> 00:29:09,030
如果你看一下用户要求模型进行预测的输入模式 就会发现有些输入比其他输入更常见

289
00:29:09,190 --> 00:29:24,390
因此，与其让模型在每次用户发出请求时都从头开始做出预测，我们先把常见的请求存储在缓存中，然后在我们对神经网络运行这个昂贵的前向传递操作之前，对缓存进行检查。

290
00:29:24,550 --> 00:29:33,950
你怎么能这么做?有很多技术可以用于智能缓存，但也有一种非常基本的方法可以做到这一点，使用Python中的函数工具库。

291
00:29:34,110 --> 00:29:43,510
这看起来像。它只是将一个包装器添加到model。predict代码中，该代码将检查缓存，看看输入是否存储在那里，如果存在，则返回这种缓存的预测。

292
00:29:43,670 --> 00:29:58,430
否则运行函数本身。这也是roblox博客文章中使用的技术之一，我之前强调过将其扩展到每天10亿个请求，这是他们方法中非常重要的一部分。因此，对于某些用例，您可以通过简单的缓存获得很大的提升。

293
00:29:58,590 --> 00:30:00,390
我们要讨论的下一个技术是批处理

294
00:30:00,550 --> 00:30:01,870
那么批处理背后的思想是什么呢

295
00:30:02,030 --> 00:30:09,170
通常，当你在机器学习模型上运行推理时，不像在训练中，你运行它的时候，它等于1。

296
00:30:09,330 --> 00:30:14,250
你有一个来自用户的请求 然后你用那个请求的预测来响应

297
00:30:14,410 --> 00:30:24,970
我们在单个请求上运行预测的事实是 一般来说 gpu在运行推理时不一定比cpu效率高的部分原因

298
00:30:25,130 --> 00:30:38,850
批处理所做的是利用gpu可以实现更高的吞吐量，更高数量的并发预测的事实，当它们并行地对批处理输入进行预测时，而不是一次对单个输入进行预测。

299
00:30:39,010 --> 00:30:42,130
这是如何工作的呢 你有来自用户的个人预测

300
00:30:42,290 --> 00:30:43,410
我想对这个输入进行预测

301
00:30:43,570 --> 00:30:44,730
我想对这个输入进行预测

302
00:30:44,890 --> 00:30:59,050
因此，您需要将这些输入收集在一起，直到您有一个足够大小的批处理，然后您将对该批处理运行预测，然后将批处理拆分为对应于各个请求的预测，并将这些预测返回给各个用户。

303
00:30:59,210 --> 00:31:01,090
这里有一些很棘手的问题

304
00:31:01,250 --> 00:31:06,190
一个是您需要调整这个批处理大小，以便在从模型获得最大吞吐量之间进行权衡。

305
00:31:06,370 --> 00:31:19,790
这通常需要更大的批处理大小，并减少用户的推理延迟，因为如果您需要等待太长时间才能收集足够的预测以适应该批处理，那么您的用户将为此付出代价。

306
00:31:19,950 --> 00:31:22,150
会有一些人在等着回应回来

307
00:31:22,310 --> 00:31:25,550
因此 您需要调整批大小 以便在这两个考虑因素之间进行权衡

308
00:31:25,710 --> 00:31:29,430
如果延迟太长，您还需要一些方法来缩短此过程。

309
00:31:29,590 --> 00:31:37,320
假设您有一个流量暂停，通常需要十分之一秒来收集128个输入，然后将其放入bash中。

310
00:31:37,480 --> 00:31:40,760
但是现在突然间它花了整整2秒来得到所有这些输入

311
00:31:40,920 --> 00:31:46,560
如果他们只能等待其他用户做出预测才能看到他们的反馈 这将是一种非常糟糕的用户体验

312
00:31:46,720 --> 00:31:54,280
所以你需要一些方法来简化收集所有数据点的过程 如果延迟对你的用户体验来说太长了

313
00:31:54,440 --> 00:32:00,640
希望从这里你们明白了这个实现起来很复杂，你们可能不希望自己实现它。

314
00:32:00,800 --> 00:32:07,310
但幸运的是 它被内置到很多库中用于在gpu上进行模型托管 我们会讲到一点

315
00:32:07,330 --> 00:32:10,390
我们将要讨论的下一个技术是在模型之间共享GPU

316
00:32:10,550 --> 00:32:22,690
这是什么意思 您的模型可能不一定充分利用您的GPU进行推理 这可能是因为您的批处理大小太小 或者是因为在等待请求时系统中有太多其他延迟

317
00:32:22,850 --> 00:32:24,690
那么为什么不建立多个模型呢

318
00:32:24,790 --> 00:32:28,330
如果您有多个模型服务在同一个视图上运行 您如何做到这一点

319
00:32:28,490 --> 00:32:34,970
这通常是相当困难的，因此这也是您希望运行开箱即用的模型服务解决方案的地方，它可以为您解决这个问题。

320
00:32:35,130 --> 00:32:48,010
我们讲过在GPU推理中 如果你想让它工作 有很多东西 比如在模型之间共享GPU和智能批处理输入到模型来权衡延迟和吞吐量你可能不想自己实现

321
00:32:48,170 --> 00:32:52,130
幸运的是，有许多库可以为您解决这些GPS托管问题。

322
00:32:52,290 --> 00:33:00,430
Tensorflow和pytorch也提供了类似的工具，前者已经很好地融入了很多谷歌云产品中，后者还有英伟达的第三方工具。

323
00:33:00,590 --> 00:33:08,230
ray nvidia的任何规模都可能是最强大的，这是我经常看到的那些试图做高吞吐量模型服务的公司。

324
00:33:08,300 --> 00:33:20,400
但是通常也很难开始使用ray serve，或者特定于你的神经网络库的ray serve，可能是一个更简单的开始方法如果你想用这个做实验的话。

325
00:33:20,500 --> 00:33:31,500
好了，我们已经讨论了如何让你的模型运行得更快如何在单个服务器上优化模型的性能，但如果你要扩展到大量用户与你的模型交互，仅从一台服务器上获得最高效率是不够的。

326
00:33:31,510 --> 00:33:39,710
在某些情况下，您需要进行水平扩展，以便让流量流向运行在不同服务器上的模型的多个副本。那么什么是水平缩放呢?

327
00:33:39,870 --> 00:33:46,470
如果一台机器的流量太大 你就会把流入的流量分成多台机器

328
00:33:46,630 --> 00:33:47,590
你如何才能真正做到这一点呢

329
00:33:47,750 --> 00:33:57,760
运行模型的每台机器都有自己独立的服务副本 然后使用称为负载平衡器的工具在这些不同副本之间路由流量

330
00:33:57,920 --> 00:34:00,440
在实践中，有两种常见的方法来做到这一点。

331
00:34:00,600 --> 00:34:05,700
一个是容器编排 这是一套技术和技术

332
00:34:05,860 --> 00:34:13,060
Kubernetes是最流行的用于管理大量不同容器的工具，这些容器作为基础设施上一个应用程序的一部分运行。

333
00:34:13,220 --> 00:34:16,740
第二种常见的方法，尤其是在机器学习中，是无服务器的。

334
00:34:16,900 --> 00:34:18,090
我们会一一讨论

335
00:34:18,200 --> 00:34:30,600
让我们从容器编排开始。当我们谈到Docker时，我们谈到了Docker与典型部署和典型虚拟机的不同之处，因为Docker不是为每个想要运行的虚拟机或程序运行单独的操作系统副本，

336
00:34:30,700 --> 00:34:40,800
相反，您可以在服务器上运行Docker，然后Docker能够管理这些轻量级虚拟机，这些虚拟机运行您想要运行的应用程序的每个部分。

337
00:34:40,850 --> 00:34:50,090
所以当你部署Docker时，通常你要做的是在服务器上运行一个Docker主机，然后你会有一堆容器，Docker主机负责管理和在服务器上运行。

338
00:34:50,250 --> 00:35:01,490
但是当您希望横向扩展时，当您希望在不同的服务器上运行应用程序的多个副本时，则需要使用不同的工具来在所有这些不同的机器和docker映像之间进行协调。

339
00:35:01,650 --> 00:35:09,790
最常见的是kubernetes, kubernetes与Docker紧密合作，构建和运行容器化的分布式应用程序。

340
00:35:10,090 --> 00:35:14,730
Kubernetes帮助您消除了所有容器都在同一台机器上运行的约束。

341
00:35:14,890 --> 00:35:21,890
Kubernetes本身是一个非常有趣的话题，如果你对分布式计算、基础设施和扩展感兴趣，值得一读。

342
00:35:22,050 --> 00:35:33,330
但是对于机器学习部署，如果您的唯一目标是部署ML模型，那么学习大量kubernetes可能是多余的。kubernetes之上构建了许多框架，这些框架使得部署模型变得更加容易。

343
00:35:33,490 --> 00:35:38,290
在实践中最常用的是kubeflow、serving和selden。

344
00:35:38,400 --> 00:35:39,300
但是，即使你在kubernetes之上使用这些库中的一个来进行容器编排，你仍然要自己负责做很多基础设施管理，

345
00:35:45,400 --> 00:35:52,600
无服务器功能是一种替代方案，它消除了对基础设施管理的大量需求，非常适合机器学习模型。

346
00:35:52,850 --> 00:36:03,980
它们的工作方式是，你把你的应用代码和依赖项打包到一个docker容器中，那个docker容器需要有一个单一的入口点函数，比如一个你会在那个容器中反复运行的函数。

347
00:36:04,140 --> 00:36:07,580
例如 在机器学习中 这通常是你的模型 预测函数

348
00:36:07,740 --> 00:36:10,550
然后把这个容器部署到一个服务上，比如aws lambda，

349
00:36:10,700 --> 00:36:24,000
或者谷歌或azure云中的类似服务，该服务负责在容器内一遍又一遍地为你运行预测功能，并负责其他所有事情，扩展，负载平衡，

350
00:36:24,100 --> 00:36:28,600
横向扩展服务器的所有其他考虑因素都是您需要解决的问题。

351
00:36:28,790 --> 00:36:34,120
最重要的是，有一个不同的定价模式，所以如果你在运行一个网络服务器，那么你就控制了整个网络服务器。

352
00:36:34,280 --> 00:36:37,120
所以你要支付它全天24小时运行的费用

353
00:36:37,280 --> 00:36:42,040
但是对于服务 您只需为这些服务器实际用于运行您的模型的时间付费

354
00:36:42,060 --> 00:36:50,480
你知道，如果你的模型只提供预测，或者每天8小时提供大部分预测，那么你就不用为其他16小时付费，或者它不提供任何预测。

355
00:36:50,640 --> 00:36:59,160
由于所有这些原因，无服务器往往非常适合于构建模型服务，特别是如果您不是基础设施专家并且希望快速入门的话。

356
00:36:59,320 --> 00:37:02,720
因此，我们建议在完成原型应用程序后，将此作为起点。

357
00:37:02,880 --> 00:37:07,760
这个绝妙的想法是如果你没有服务器，你的服务器不会宕机我们做无服务器。

358
00:37:07,920 --> 00:37:18,440
无服务器并非没有缺点，其中一个更大的挑战最近变得更容易了，但在实践中仍然是一个挑战，那就是您可以使用这些无服务器应用程序部署的包往往在大小上受到限制。

359
00:37:18,600 --> 00:37:21,880
所以如果你有一个绝对庞大的模型 你可能会遇到这些限制

360
00:37:22,040 --> 00:37:23,080
还有一个冷启动问题。

361
00:37:23,240 --> 00:37:26,880
这意味着无服务器被设计成可以一直缩小到零。

362
00:37:27,040 --> 00:37:34,320
因此，如果你没有收到任何流量，如果你没有收到任何对你的模型的请求，那么你就不会付钱，这是无服务器的一大优势。

363
00:37:34,480 --> 00:37:40,800
但问题是，当您获得第一个请求时，在调用无服务器函数一段时间后，它需要一段时间才能启动。

364
00:37:40,820 --> 00:37:44,440
可能需要几秒钟甚至几分钟才能恢复第一次预测

365
00:37:44,600 --> 00:37:50,650
一旦你得到了第一个预测 得到后续的预测会更快 但仍然值得注意这个限制

366
00:37:50,810 --> 00:37:57,770
另一个挑战，实际的挑战，是许多这些无服务器服务没有很好地设计用于构建管道和模型。

367
00:37:57,930 --> 00:38:03,930
因此，如果您有一个复杂的逻辑训练来生成您的预测，那么在无服务器上下文中实现它可能会很困难。

368
00:38:04,090 --> 00:38:08,210
在无服务器功能中很少或根本没有可用的状态管理。

369
00:38:08,370 --> 00:38:18,890
因此，例如，如果缓存对你的应用程序非常重要，如果你在无服务器中部署你的模型，那么构建缓存可能会很困难，而且通常部署工具也很有限。

370
00:38:19,050 --> 00:38:24,490
因此，推出无服务器功能的新版本时，通常不会有您想要的所有工具。

371
00:38:24,650 --> 00:38:32,850
最后，今天这些无服务器的功能只有CPU，它们的融合时间有限，你知道，几秒钟或几分钟。

372
00:38:33,010 --> 00:38:37,120
因此，如果您确实需要gpu来处理印迹，那么无服务器将不是您的答案。

373
00:38:37,280 --> 00:38:39,080
但我不认为限制是永远正确的

374
00:38:39,240 --> 00:38:41,840
事实上，我认为我们可能离无服务器gpu很近了。

375
00:38:42,000 --> 00:38:46,200
已经有几家初创公司声称要提供无服务器Gpu。

376
00:38:46,360 --> 00:38:54,240
所以 如果你想在GPU上做推理 但你不想自己管理GPU机器 我建议你看看这两个年轻的初创公司的选择

377
00:38:54,400 --> 00:39:00,320
构建模型服务的下一个主题是推出，所以在推出新模型时需要考虑什么?

378
00:39:00,480 --> 00:39:11,320
如果服务是你如何把你的机器模型变成可以响应请求的东西，它存在于web服务器上，任何人，或者任何你想要的人都可以向它发送请求并得到预测结果。

379
00:39:11,480 --> 00:39:14,680
然后 部署是管理和更新这些服务的方式

380
00:39:14,840 --> 00:39:21,520
如果你有一个新版本的模型 或者如果你想在两个不同版本之间分割流量来运行a-b测试 你该怎么做呢

381
00:39:21,580 --> 00:39:25,640
从基础设施的角度来看 您可能希望能够做一些不同的事情

382
00:39:25,800 --> 00:39:28,080
一是逐步推出新版本

383
00:39:28,200 --> 00:39:37,500
这意味着，当你的模型有n + 1版本你想要替换它的版本时，有时能够，而不是立即将所有流量切换到n + 1，

384
00:39:37,500 --> 00:39:46,100
相反，首先将1%的流量发送给n + 1，然后是10%，然后是50%，然后一旦你确信它运行良好，然后将所有流量切换到它。

385
00:39:46,280 --> 00:39:48,240
所以你希望能够逐步推出新版本

386
00:39:48,400 --> 00:39:51,520
另一方面 您希望能够立即回滚到旧版本

387
00:39:51,680 --> 00:40:01,040
如果你检测到你部署的新版本模型的问题 嘿 在我发送新模型的这10%的流量上 用户没有很好地响应它 或者它发送了一堆错误

388
00:40:01,200 --> 00:40:05,120
您希望能够立即恢复将所有流量发送到旧版本的模型

389
00:40:05,280 --> 00:40:08,840
您希望能够在不同版本之间分割流量 这是做这些事情的先决条件

390
00:40:09,000 --> 00:40:16,960
除了运行a-b测试之外 您还希望能够部署模型管道 或者以一种能够遮蔽预测流量的方式部署模型

391
00:40:17,020 --> 00:40:26,040
它们可以查看与主模型相同的输入 并产生不会发送给用户的预测 这样您就可以在开始向用户展示预测之前测试预测是否合理

392
00:40:26,200 --> 00:40:31,920
这只是简单介绍了一些在模型推出中可能需要解决的问题。

393
00:40:32,080 --> 00:40:36,280
这是一个具有挑战性的基础设施问题 所以它真的超出了这堂课的范围

394
00:40:36,440 --> 00:40:46,640
如果您使用的是托管选项，稍后会用到，或者您的团队为您提供了基础设施，它可能已经为您解决了这个问题，但如果没有，那么研究托管选项可能是一个好主意。

395
00:40:46,800 --> 00:41:00,380
所以管理选项处理了很多扩展和推出的挑战，如果你自己托管模型，你会面临这些挑战，即使是在像aws lambda这样的东西上，这里有一些不同类别的选项。

396
00:41:00,540 --> 00:41:06,640
云提供商都提供了自己的托管选项，以及大多数端到端ML平台。

397
00:41:06,800 --> 00:41:14,400
因此，如果您已经大量使用这些云提供商或端到端ML平台之一，那么值得检查他们的产品，看看是否适合您。

398
00:41:14,560 --> 00:41:17,560
也有很多创业公司在这里提供服务

399
00:41:17,720 --> 00:41:23,270
我想说，有几家公司更注重开发者体验，比如Bento ml和Cortex。

400
00:41:23,680 --> 00:41:25,070
如果你觉得sagemaker很难用，

401
00:41:25,280 --> 00:41:29,400
或者你只是讨厌它的开发者体验，这可能值得一试。

402
00:41:29,560 --> 00:41:34,440
Cortex最近被databricks收购了，所以它可能也会开始更多地融入他们的产品中。

403
00:41:34,600 --> 00:41:40,000
还有一些初创公司的产品更容易使用 但也非常注重性能

404
00:41:40,160 --> 00:41:42,920
香蕉就是一个很受欢迎的例子。

405
00:41:43,080 --> 00:41:58,400
为了让你感觉一下这些管理选项是什么样的，我想双击sagemaker，这可能是最流行的管理，如果你的模型已经是一个易于理解的格式，一个拥抱脸模型，或者一个科学工具包学习模型，或者类似的东西，在sagemaker中提供快乐路径。

406
00:41:58,560 --> 00:42:01,000
在这些情况下，指向sagemaker是很容易的。

407
00:42:01,160 --> 00:42:09,280
你不会使用基本的拥抱脸类，你会使用这个sagemaker包装器来记录脸类，然后像平常一样调用fit。

408
00:42:09,380 --> 00:42:10,520
它也可以在云上运行

409
00:42:10,680 --> 00:42:20,480
然后要部署，只需调用拥抱面包装器的。deploy方法，并指定要在多少个实例上运行，以及运行它所需的硬件有多强大。

410
00:42:20,640 --> 00:42:28,080
然后你可以调用predictor。使用一些输入数据进行预测，它会在云端为你运行预测，以便返回你的响应。

411
00:42:28,240 --> 00:42:33,040
你知道，我想说的是，在过去，sagemaker以难以使用而闻名，如果你只是在做推理。

412
00:42:33,200 --> 00:42:35,120
我不认为这样的名声是合理的

413
00:42:35,280 --> 00:42:45,520
我认为它实际上很容易使用，在很多情况下是部署模型的一个很好的选择，因为它有很多简单的包装器来防止你需要构建自己的docker容器或类似的东西。

414
00:42:45,680 --> 00:42:53,520
它提供了将模型部署到专用web服务器(如您在本例中看到的)以及部署到无服务器实例的选项。

415
00:42:53,600 --> 00:43:00,800
使用SageMaker的主要缺点是:如果你想做一些比标准的拥抱脸或scikit- learn模型更复杂的事情，

416
00:43:00,800 --> 00:43:08,000
您仍然需要部署容器，并且用于部署容器的界面可能不像您希望的那样用户友好或简单。

417
00:43:08,200 --> 00:43:15,500
有趣的是，截至昨天，将模型应用于专用实例的成本比原始ec2要高得多。

418
00:43:15,500 --> 00:43:25,600
如果你打算采用无服务器模式，你愿意支付一定比例的开销，以获得部署大多数机器学习模型比Sagemaker更好的体验。

419
00:43:25,700 --> 00:43:29,900
值得一试。如果你已经在亚马逊上了，从建立一个模型服务开始，你可能不需要做GPU推理，

420
00:43:29,900 --> 00:43:43,800
如果你在做CPU推理，那么通常水平扩展到更多的服务器，或者甚至只是使用无服务器是最简单的选择。

421
00:43:44,090 --> 00:43:48,330
如果您可以使用cpu，那么无服务器可能是推荐的选项。

422
00:43:48,400 --> 00:43:50,000
如果你的流量比较大，这尤其有用，如果你早上有更多的用户，或者你只在晚上发送你的模型预测，

423
00:43:50,000 --> 00:44:03,500
或者，如果你的流量很低，你不会让一个完整的强大的web服务器满负荷，sagemaker越来越成为一个完美的开始方式。

424
00:44:03,690 --> 00:44:05,650
如果你是A-W-S 可能会很贵

425
00:44:05,810 --> 00:44:09,610
一旦你到了成本真的开始影响的地步，你就会考虑其他选择。

426
00:44:09,770 --> 00:44:14,890
如果您决定沿着GPU推理的路线走下去 那么不要尝试滚动您自己的GPU推理

427
00:44:15,050 --> 00:44:22,490
相反，值得投资使用像tensorflow服务或triton这样的工具，因为这些最终将节省您的时间并最终带来更好的性能。

428
00:44:22,650 --> 00:44:33,510
最后，我认为有必要关注一下这个领域中按需GPU推理的初创公司，因为我认为这可能会改变GPU推理对于机器学习模型是否真的值得的等式。

429
00:44:33,670 --> 00:44:41,510
下一个主题是将模型完全移出web服务器 并将其推向边缘 将其推向用户所在的位置

430
00:44:41,670 --> 00:44:43,150
你应该什么时候开始考虑这个问题

431
00:44:43,310 --> 00:44:49,620
有时候这是显而易见的 假设你的用户没有可靠的互联网连接 他们在沙漠中驾驶一辆自动驾驶汽车

432
00:44:49,700 --> 00:45:02,500
或者如果你有非常严格的数据安全或隐私要求，如果你在苹果设备上构建，你不能将你需要的数据发送回网络服务器，

433
00:45:02,500 --> 00:45:07,600
否则，如果您没有这些严格的要求，那么您需要考虑的权衡是模型的准确性和用户接收该模型响应的延迟。

434
00:45:07,680 --> 00:45:11,560
这是我们最终关心的事情 那就是建立一个良好的用户体验

435
00:45:11,740 --> 00:45:13,620
延迟有几个不同的组成部分。

436
00:45:13,780 --> 00:45:19,680
它的一个组成部分是模型自己做出预测所需的时间，另一个组成部分是网络往返。

437
00:45:19,840 --> 00:45:27,830
所以用户的请求到达你的模型服务需要多长时间 以及预测返回到你的用户运行的客户端设备需要多长时间

438
00:45:27,900 --> 00:45:38,600
所以如果你已经用尽了所有的方法来减少它们做出预测所需的时间，或者如果你的要求非常严格，以至于你没有办法在你的延迟之内

439
00:45:38,600 --> 00:45:50,900
sla只是通过减少模型进行预测所需的时间，那么值得考虑迁移到边缘，即使您有可靠的互联网连接并且没有非常严格的数据安全和隐私要求。

440
00:45:51,110 --> 00:45:56,050
但值得注意的是，移动到边缘会增加很多复杂性，这在web开发中是不存在的。

441
00:45:56,210 --> 00:45:58,050
所以仔细想想你是否真的需要这个

442
00:45:58,210 --> 00:46:06,800
这是我们在EdgePrediction中考虑的模型 其中模型本身在客户端设备上运行 而不是在服务器或其自己的服务中运行

443
00:46:06,960 --> 00:46:12,160
其工作方式是 您将把废物发送到客户端设备 然后客户端将加载模型并直接与之交互

444
00:46:12,180 --> 00:46:14,360
这种方法有利有弊

445
00:46:14,520 --> 00:46:19,090
最大的好处是 这是构建机器学习功能产品的最低延迟方式

446
00:46:19,250 --> 00:46:22,810
延迟通常是一个非常重要的用户体验驱动因素。

447
00:46:22,970 --> 00:46:24,330
它不需要互联网连接

448
00:46:24,490 --> 00:46:29,530
因此，如果你正在构建机器人或其他类型的设备，你想在上面运行ml。

449
00:46:29,690 --> 00:46:30,510
这是一个很好的选择

450
00:46:30,810 --> 00:46:35,690
这对于数据安全来说是非常好的，因为需要进行预测的数据永远不需要离开用户的设备。

451
00:46:35,800 --> 00:46:36,400
从某种意义上说，你可以免费获得扩展，因为你不需要考虑如何扩展我的网络服务来满足所有用户的需求，

452
00:46:36,400 --> 00:46:52,900
每个用户都将带来自己的硬件，用于运行模型的预测，因此您不需要考虑如何增加和减少运行模型推理所需的资源。

453
00:46:53,090 --> 00:46:55,090
这种方法也有一些非常明显的缺点

454
00:46:55,250 --> 00:47:00,290
首先 在这些边缘设备上 可用的硬件资源通常非常有限

455
00:47:00,300 --> 00:47:11,600
所以，如果你习惯于在强大的现代gpu机器上运行你的每一个模型预测，那么当你试图让你的模型在你需要的设备上工作时，你将会感到有点震惊。

456
00:47:11,810 --> 00:47:23,090
你用来让模型在有限的硬件上运行的工具功能不全，而且在很多情况下更难使用，比你可能习惯使用的神经网络库更容易出现错误和bug。

457
00:47:23,250 --> 00:47:29,530
在tensorflow和pytorch中，由于您需要将更新的模型权重发送到设备，因此更新模型可能非常困难。

458
00:47:29,690 --> 00:47:32,930
在web部署中 您可以完全控制部署模型的哪个版本

459
00:47:33,090 --> 00:47:34,970
如果有一个bug 你可以很快地修复它

460
00:47:35,130 --> 00:47:44,500
但在边缘 你需要更仔细地考虑你的策略 以更新你的用户在他们的设备上运行的模型的版本 因为他们可能并不总是能够得到最新的模型

461
00:47:44,600 --> 00:47:51,600
最后，当事情出错时，如果你的模型出现了错误，很难检测到这些错误并修复和调试它们，

462
00:47:51,600 --> 00:48:00,400
因为作为模型开发人员，你没有经过模型的原始数据，因为这些数据都在用户的设备上。

463
00:48:00,620 --> 00:48:04,820
接下来，我们将快速介绍一下可以用于边缘部署的不同框架。

464
00:48:04,980 --> 00:48:10,780
选择正确的框架既取决于你如何训练你的模型 也取决于你想把它部署在什么目标设备上

465
00:48:10,940 --> 00:48:20,500
所以我们不打算对这些选择中的任何一个进行特别深入的研究，而只是给你一个大体的画面，让你在做这个决定时可以考虑哪些选择。

466
00:48:20,660 --> 00:48:23,140
所以我们会根据你部署到的设备来划分。

467
00:48:23,300 --> 00:48:29,300
最简单的答案是，如果你部署到nvidia设备上，那么正确的答案可能是tensorsort。

468
00:48:29,460 --> 00:48:36,340
所以无论是GPU，比如你训练模型用的GPU，还是英伟达专门设计部署在边缘上的设备。

469
00:48:36,500 --> 00:48:38,300
张排序趋向于这里的选项。

470
00:48:38,400 --> 00:48:41,100
如果你不是部署到nvidia设备上，而是部署到手机上，那么Android和Apple都有用于在其特定操作系统上部署神经网络的库，如果你知道你只会部署到Apple设备或Android设备上，这是一个不错的选择，

471
00:48:41,100 --> 00:49:06,400
但如果你正在使用pytorch，并且你希望能够同时在ios和Android上部署，那么你可以查看pytorch mobile，它将pytorch编译成可以在这两个操作系统上运行的东西。

472
00:49:06,620 --> 00:49:17,320
同样，tensorflow lite旨在使tensorflow在不同的移动操作系统以及其他既不是移动设备也不是英伟达设备的边缘设备上工作。

473
00:49:17,480 --> 00:49:33,420
如果你不是部署到nvidia设备，不是部署到手机，也不是部署到你可能考虑的其他边缘设备，而是部署到浏览器，出于性能或可扩展性或数据隐私的原因，那么tensorflow.js可能是这里要考虑的主要示例。

474
00:49:33,580 --> 00:49:36,500
我不知道部署pytorch到浏览器的好选择。

475
00:49:36,660 --> 00:49:40,260
最后，你可能会想，为什么有这么多的选择?

476
00:49:40,420 --> 00:49:47,180
就像我需要遵循这个复杂的决策树来选择一些东西，这取决于我训练模型的方式，我要部署它的目标设备

477
00:49:47,340 --> 00:49:54,020
甚至没有很好的方法来填充图中的一些单元格，比如，如何在非手机的边缘设备上运行pytorch模型?

478
00:49:54,180 --> 00:49:55,500
这可能不是很清楚

479
00:49:55,560 --> 00:50:07,560
在这种情况下，可能值得研究一下这个名为apache TVM的库，apache TVM旨在成为一个库无关和目标无关的工具，用于将您的模型编译成可以在任何地方运行的东西。

480
00:50:07,640 --> 00:50:10,220
这个想法是在任何地方建立模型 在任何地方运行它

481
00:50:10,380 --> 00:50:20,180
apache tvm已经得到了一些采用，但我想说，在这一点上，它离成为行业标准还有很长的路要走，但如果您需要使您的模型在许多不同类型的设备上工作，那么它是一个值得研究的选项。

482
00:50:20,340 --> 00:50:22,220
最后，我想说，注意这个空间。

483
00:50:22,240 --> 00:50:26,820
我认为这是机器学习创业公司发展的另一个非常活跃的领域

484
00:50:26,980 --> 00:50:31,580
特别是，围绕apache tvm有一个名为Octoml的启动程序，值得研究一下。

485
00:50:31,700 --> 00:50:37,200
有一个新的创业公司是由低级库mlir的开发人员建立的，叫做模块化，

486
00:50:37,300 --> 00:50:45,600
它的目标是潜在地解决边缘部署的一些问题，以及谷歌的一个项目tiny ml。

487
00:50:45,830 --> 00:50:54,930
我们讨论了在边缘上运行模型时可以使用的框架，但这些框架只会在模型太大而无法放到边缘上的情况下发挥作用。

488
00:50:55,310 --> 00:50:58,010
所以我们需要创造更有效的模式

489
00:50:58,170 --> 00:51:00,530
在前一节中，我们讨论了量子化和蒸馏。

490
00:51:00,690 --> 00:51:04,650
这两种技术对于设计这类模型都很有帮助

491
00:51:04,810 --> 00:51:10,410
但也有专门设计用于移动或边缘设备的模型架构

492
00:51:10,570 --> 00:51:13,660
最有效的例子就是移动网络

493
00:51:13,820 --> 00:51:24,000
移动网络的想法是在典型的比较网络中采用一些昂贵的操作，比如具有更大过滤器尺寸的卷积层，并用更便宜的操作取代它们，比如1x1卷积。

494
00:51:24,180 --> 00:51:33,420
所以值得去看看这篇移动网络论文如果你想学习更多关于移动网络的知识，也许会得到灵感如何为你的问题设计一个移动友好的架构。

495
00:51:33,440 --> 00:51:36,100
特别是MobileNets，它是一个非常好的移动部署工具。

496
00:51:36,260 --> 00:51:40,060
相对于更大的模型，它们在准确性方面往往没有很大的权衡，

497
00:51:40,180 --> 00:51:41,710
但是它们是

498
00:51:41,840 --> 00:51:46,160
更小，更容易安装在边缘设备上。

499
00:51:46,420 --> 00:51:49,800
我推荐的另一个案例研究是研究蒸馏酒

500
00:51:50,000 --> 00:51:55,500
蒸馏器是模型蒸馏的一个例子，它能很好地得到一个更小的蒸馏器，省去了一些更昂贵的操作

501
00:51:55,600 --> 00:52:03,900
使用模型蒸馏得到的模型性能并不比bert差多少，但占用的空间更少，运行速度更快。

502
00:52:04,200 --> 00:52:10,800
所以，为了结束我们对边缘部署的讨论，我想谈谈我学到的边缘部署的一些关键心态，

503
00:52:10,900 --> 00:52:16,000
与一群在边缘部署机器学习模型方面比我经验丰富的从业者交谈。

504
00:52:16,180 --> 00:52:22,220
第一个是，我认为有一种诱惑，首先要找到完美的模型，架构，然后弄清楚如何让它在你的设备上工作。

505
00:52:22,380 --> 00:52:28,180
通常情况下 如果你在一个web服务器上 你可以让这个工作 因为你总是可以选择横向扩展

506
00:52:28,340 --> 00:52:31,980
所以如果你有一个巨大的模型 运行起来可能会很昂贵 但你仍然可以让它工作

507
00:52:32,140 --> 00:52:38,940
但在边缘上 从业者认为最好的做法是在选择体系结构时考虑到目标硬件

508
00:52:39,100 --> 00:52:43,820
因此 您不应该考虑无法在您的设备上运行的架构

509
00:52:43,980 --> 00:52:56,860
一个经验法则是你可能能够弥补一个因素，比如说，一个数量级，2到10倍的推理时间或模型大小，通过一些蒸馏，量化和其他技巧的组合。

510
00:52:57,020 --> 00:52:59,340
但通常情况下 你不会得到超过10倍的奖励

511
00:52:59,500 --> 00:53:05,900
因此 如果您的模型太大或太慢 无法在目标上下文中运行 那么您可能甚至不应该考虑该体系结构

512
00:53:06,000 --> 00:53:14,600
下一个思路是，一旦你有了一个可以在你的边缘设备上运行的模型版本，你就可以在本地迭代，而不必测试你在那个设备上所做的所有更改，

513
00:53:14,700 --> 00:53:20,500
这真的很有帮助，因为在边缘部署和测试本身就很棘手，而且可能很昂贵。

514
00:53:20,660 --> 00:53:29,700
但是你可以在你迭代的版本有效时进行局部迭代 只要你只是逐渐增加模型的大小或模型的延迟

515
00:53:29,800 --> 00:53:37,400
从业者建议做的一件事是，我认为，如果你要这样做，一个值得采取的步骤是添加指标，或者添加模型大小和延迟的测试，

516
00:53:37,400 --> 00:53:43,400
如果你在局部迭代时你有点忘乎所以你把模型的大小扩大了一倍或三倍，

517
00:53:43,400 --> 00:53:51,700
你至少会有一个测试提醒你，你可能需要再次检查以确保这个模型会在我们需要运行的设备上运行。

518
00:53:51,890 --> 00:54:02,730
我从边缘部署实践者那里学到的另一种思维方式是 将设备的模型调优视为模型部署生命周期中的额外风险 并相应地进行测试

519
00:54:02,890 --> 00:54:08,710
因此 例如 在实际将模型部署到生产硬件之前 总是在生产硬件上测试模型

520
00:54:08,950 --> 00:54:14,790
现在，这似乎是显而易见的，但在实践中这并不是最容易做到的事情，所以一些不熟悉边缘部署的人会跳过这一步。

521
00:54:14,900 --> 00:54:21,500
这一点之所以重要是因为，由于这些可食用的极限库还不成熟，

522
00:54:21,500 --> 00:54:28,600
神经网络在边缘设备上的工作方式与在训练设备或笔记本电脑上的工作方式通常会有细微的差异。

523
00:54:28,830 --> 00:54:39,780
因此，重要的是在边缘设备上运行模型的预测函数，在一些基准数据集上测试模型在特定硬件上的延迟和准确性，然后再部署它。

524
00:54:39,940 --> 00:54:50,110
否则 您的模型在该硬件上的工作方式与在您的开发环境中的工作方式之间的差异可能会导致其他已部署模型的不可预见的错误或不可预见的准确性降低

525
00:54:50,200 --> 00:55:02,200
最后，由于机器学习模型通常是非常挑剔的，所以在应用程序中建立回退机制是一个好主意，以防模型失败，或者你不小心推出了一个糟糕的模型版本，

526
00:55:02,200 --> 00:55:09,700
或者模型运行太慢，无法为用户解决任务，这些回退机制看起来就像你的模型的早期版本，

527
00:55:09,700 --> 00:55:16,300
更简单或更小的模型将会是可靠的并且在它们需要运行的时间内运行。

528
00:55:16,460 --> 00:55:26,500
或者甚至就像基于规则的函数，如果你的模型花了很长时间来做出预测，或者出错了，你仍然有一些东西会返回一个响应给你的最终用户。

529
00:55:26,660 --> 00:55:32,820
因此，为了结束我们对边缘部署的讨论，首先要提醒你的是，web部署确实比边缘部署容易得多。

530
00:55:32,980 --> 00:55:35,180
所以只有在真正需要的时候才使用边缘部署。

531
00:55:35,300 --> 00:55:44,200
第二，你需要选择一个框架来进行边缘部署，你要做到这一点的方法是将你用来构建神经网络的库和可用的硬件相匹配，

532
00:55:44,200 --> 00:55:48,300
选择与这两个约束相匹配的相应边缘部署框架。

533
00:55:48,500 --> 00:55:55,260
如果您想要更加灵活，例如，如果您希望您的模型能够在多个设备上工作，那么值得考虑使用apache TVM之类的东西。

534
00:55:55,420 --> 00:56:00,380
第三，开始考虑在项目开始时从边缘部署中获得的额外约束。

535
00:56:00,540 --> 00:56:06,860
不要等到你投入了三个月的时间来开发一个完美的模型 才去考虑这个模型是否真的能够在边缘上运行

536
00:56:07,020 --> 00:56:15,670
相反 请确保从第一天开始就考虑到边缘部署的这些限制 并相应地选择架构和培训方法

537
00:56:15,830 --> 00:56:18,670
结束我们关于全面部署机器学习模型的讨论

538
00:56:18,830 --> 00:56:29,310
建立模型是构建机器学习强大产品的必要步骤 但它也非常有用 可以让你的模型变得更好 因为只有在现实生活中 才能看到你的模型如何在我们真正关心的任务上工作

539
00:56:29,470 --> 00:56:35,670
所以我们鼓励你在这里尽早地部署这种心态 这样你就可以尽快地从现实世界中收集反馈

540
00:56:35,830 --> 00:56:42,710
保持简单 只在需要时添加复杂性 因为这个部署可能是一个兔子洞(漏洞百出) 这里有很多复杂性需要处理

541
00:56:42,870 --> 00:56:44,550
所以要确保你真的需要这种复杂性

542
00:56:44,710 --> 00:56:45,750
所以从建立一个原型开始

543
00:56:45,990 --> 00:56:47,550
一旦你需要开始扩大规模

544
00:56:47,790 --> 00:56:52,910
然后通过批处理预测或构建模型服务将模型与UI分离。

545
00:56:52,970 --> 00:56:53,030
然后

546
00:56:53,470 --> 00:57:04,870
一旦你以一种幼稚的方式部署了你的模型，停止了扩展，那么你可以学习扩展的技巧，或者使用托管服务或云提供商选项来为你处理大量的扩展。

547
00:57:04,800 --> 00:57:11,300
最后，如果你真的需要能够在一个不能持续上网的设备上操作你的模型，

548
00:57:11,300 --> 00:57:18,300
如果您有非常严格的数据安全要求，或者如果您真的、真的、真的想要快速运行，那么请考虑将您的模型移动到边缘。

549
00:57:18,470 --> 00:57:24,040
但请注意 这将增加很多复杂性 并迫使您在想要这样做时使用一些不太成熟的工具

550
00:57:24,200 --> 00:57:27,830
这节关于部署的课就到这里 下周见
